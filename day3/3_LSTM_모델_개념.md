# LSTM 모델 개념

딥러닝, 특히 자연어 처리나 시계열 데이터 분석을 공부하다 보면 반드시 마주하게 되는 산이 바로 **LSTM(Long Short-Term Memory)** 입니다.

기존의 일반적인 RNN(Recurrent Neural Network)이 가진 치명적인 약점을 보완하기 위해 등장한 모델입니다.

---

## 1. LSTM이 왜 등장했나요? (RNN의 한계)

일반적인 RNN은 정보가 전달될수록 앞부분의 정보가 소실되는 **'장기 의존성(Long-Term Dependency)'** 문제를 가지고 있습니다. 문장이 길어지면 앞쪽 단어의 의미를 뒤쪽까지 전달하지 못하고 잊어버립니다.

이를 수학적으로는 **Gradient Vanishing(기울기 소실)** 문제라고 부릅니다. LSTM은 이를 해결하기 위해 "기억할 것은 오래 기억하고, 잊을 것은 과감히 잊자"는 전략을 취합니다.

---

## 2. LSTM의 핵심 구조: 3개의 문(Gate)

LSTM의 핵심은 **셀 상태(Cell State)** 라는 일종의 '컨베이어 벨트'입니다. 이 벨트 위에 정보를 태워 보내면서, 세 개의 '문(Gate)'을 통해 어떤 정보를 더하고 뺄지 결정합니다.

### ① 삭제 게이트 (Forget Gate)

* **역할:** 과거의 정보 중 무엇을 버릴지 결정합니다.
* 현재 입력값($x_t$)과 이전 은닉 상태($h_{t-1}$)를 보고 시그모이드 함수($\sigma()$)를 거쳐 0(완전 삭제)과 1(완전 유지) 사이의 값을 출력합니다.
* 시그모이드 함수($\sigma()$)는 모든 값을 0(차단)과 1(통과) 사이로 변환하여, 인공지능이 "얼마나 정보를 전달할지"를 결정하게 돕는 필터생각하면 됩니다.

### ② 입력 게이트 (Input Gate)

* **역할:** 새로 들어온 정보 중 무엇을 셀 상태에 저장할지 결정합니다.
* 중요한 정보는 저장하고, 불필요한 정보는 걸러냅니다.

### ③ 출력 게이트 (Output Gate)

* **역할:** 업데이트된 셀 상태를 바탕으로 어떤 값을 외부(다음 층)로 내보낼지 결정합니다.

---

## 3. LSTM의 수학적 메커니즘

LSTM 내부의 복잡한 계산 과정을 수식으로 표현하면 다음과 같습니다. (여기서 $\sigma$는 시그모이드 함수, $\tanh$는 하이퍼볼릭 탄젠트 함수를 의미합니다.)

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$
$$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t * \tanh(C_t)$$

이 수식들이 유기적으로 작동하면서 데이터의 **장기 문맥**을 파악하게 됩니다.

---

## 4. LSTM 요약

* **RNN의 진화형:** 긴 시퀀스 데이터를 처리할 때 발생하는 정보 손실 문제를 해결했습니다.
* **Cell State:** 정보가 흐르는 중심 통로 역할을 합니다.
* **Gating Mechanism:** '삭제-입력-출력'이라는 세 단계를 통해 효율적으로 메모리를 관리합니다.

최근에는 LSTM보다 구조가 단순한 **GRU**나, 아예 구조가 다른 **Transformer(Attention 기반)** 모델들이 많이 쓰이기도 하지만, LSTM은 여전히 시계열 예측 등 다양한 분야에서 기본이 되는 매우 중요한 모델입니다.

---

## 5. 시계열 예측에서 LSTM이 중요한 이유

시계열 예측(Time-Series Forecasting)에서 LSTM이 '게임 체인저'로 불리는 이유는 데이터가 가진 **시간적 문맥(Temporal Context)** 을 가장 잘 이해하기 때문입니다.

일반적인 머신러닝 모델은 데이터 포인트 하나하나를 독립적으로 보지만, LSTM은 데이터 사이의 흐름과 패턴을 포착합니다.

---

### 1. 비선형적 장기 패턴 포착 (Capturing Non-linear Long-term Patterns)

시계열 데이터는 단순히 직전의 값에만 영향을 받지 않습니다. 예를 들어 주식 시장이나 전력 수요는 **며칠 전, 혹은 몇 달 전의 주기적 특성**이 현재에 영향을 줍니다.

* **이유:** LSTM의 '셀 상태'는 수백 개 이전 시점의 중요한 정보를 현재로 끌고 올 수 있습니다.
* **효과:** 계절성(Seasonality)이나 장기적인 추세(Trend)를 모델이 스스로 학습하여 복잡한 비선형 관계를 파악합니다.

---

### 2. 동적인 시차 반영 (Dynamic Time Lag)

전통적인 시계열 모델인 ARIMA 등은 "과거 $n$개 시점의 데이터를 보겠다"라고 고정(Lag)해야 합니다. 하지만 실제 세계의 사건은 영향력이 나타나는 시간이 제각각입니다.

* **이유:** LSTM의 **망각 게이트**는 과거 정보가 언제까지 유효한지 동적으로 결정합니다.
* **효과:** 어떤 정보는 10분만 기억하고, 어떤 정보는 10일 동안 유지하는 식의 유연한 예측이 가능해집니다.

---

## 3. 다변량 데이터 처리 능력 (Multivariate Time-series)

시계열 예측은 종종 한 가지 변수만 쓰지 않습니다. (예: 기온 예측 시 습도, 풍속, 기압 등을 동시에 고려)

* **이유:** LSTM은 여러 입력 변수를 동시에 받아들여, 각 변수들이 서로의 미래 값에 어떤 영향을 주는지 병렬적으로 학습할 수 있습니다.
* **효과:** 상관관계가 복잡한 여러 지표를 통합하여 더 정확한 예측치를 산출합니다.

---

## 4. 데이터 노이즈에 대한 저항성

시계열 데이터에는 예측을 방해하는 '노이즈(Noise)'가 섞여 있기 마련입니다.

* **이유:** LSTM은 게이트 구조를 통해 갑작스럽게 튀는 이상치(Outlier)나 무의미한 변동을 '망각'하거나 무시하도록 학습될 수 있습니다.
* **효과:** 데이터의 자잘한 흔들림에 일희일비하지 않고 핵심적인 흐름(Signal)을 따라가는 안정적인 예측 결과를 제공합니다.

---

## 시계열 데이터 처리 흐름 비교

| 모델 종류 | 특징 | 시계열 데이터에서의 한계 |
| --- | --- | --- |
| **회귀 분석 (Regression)** | 독립적인 데이터 처리 | 데이터의 순서(Order) 정보를 완전히 무시함 |
| **ARIMA 계열** | 통계적 추세 기반 | 선형적인 패턴만 잘 잡으며, 장기 의존성 처리가 어려움 |
| **LSTM** | **순환 신경망(RNN) 기반** | **데이터의 전후 맥락과 장기 패턴을 동시에 학습 가능** |

---

**요약** LSTM은 데이터 속에 숨겨진 **'시간의 흐름에 따른 인과관계'** 를 가장 정교하게 모사할 수 있는 구조이기 때문에 시계열 분야에서 필수적인 모델로 자리 잡았습니다.
