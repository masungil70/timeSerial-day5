import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import koreanize_matplotlib # í•œê¸€ í°íŠ¸ ì„¤ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. matplotlibì—ì„œ í•œê¸€ì´ ê¹¨ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•´ì¤ë‹ˆë‹¤.
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

plt.rcParams['axes.unicode_minus'] = False 
plt.rcParams['figure.dpi'] = 150  # ê³ í•´ìƒë„ ì¶œë ¥
plt.rcParams['lines.antialiased'] = True # ì„  ë¶€ë“œëŸ½ê²Œ ì„¤ì • ê°•ì œí™”

# 1. ì¥ì¹˜ ì„¤ì • (RTX 50xx í™•ì¸)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜: {device}")

# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
df = pd.read_csv('./data/power_usage_dataset_3month.csv')
df['Date'] = pd.to_datetime(df['Date'])

# ì‹œê°„ ì£¼ê¸°ì„± ë°˜ì˜ (íŠ¹ì„± ê³µí•™)
df['hour_sin'] = np.sin(2 * np.pi * df['Date'].dt.hour / 23)
df['hour_cos'] = np.cos(2 * np.pi * df['Date'].dt.hour / 23)
df['weekday_sin'] = np.sin(2 * np.pi * df['Date'].dt.weekday / 6)
df['weekday_cos'] = np.cos(2 * np.pi * df['Date'].dt.weekday / 6)

features_list = ['Temperature', 'Usage', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos']
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[features_list].values)

# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_sequences(data, window_size=168):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i + window_size, :]) 
        y.append(data[i + window_size, 1]) # Target: Usage (index 1)
    return np.array(X), np.array(y)

window_size = 168
X, y = create_sequences(scaled_data, window_size)

# ë°ì´í„° ë¶„í•  ë° Tensor ë³€í™˜
split = int(len(X) * 0.8)
X_train = torch.FloatTensor(X[:split]).to(device)
y_train = torch.FloatTensor(y[:split]).to(device)
X_test = torch.FloatTensor(X[split:]).to(device)
y_test = torch.FloatTensor(y[split:]).to(device)

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)

# 3. ëª¨ë¸ ì„¤ê³„ (Stacked LSTM)
class StackedLSTM(nn.Module):
    def __init__(self, input_size):
        super(StackedLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, 128, batch_first=True)
        self.dropout1 = nn.Dropout(0.2)
        self.lstm2 = nn.LSTM(128, 64, batch_first=True)
        self.dropout2 = nn.Dropout(0.1)
        self.fc1 = nn.Linear(64, 16)
        self.fc2 = nn.Linear(1, 1) # ìµœì¢… ì¶œë ¥

    def forward(self, x):
        # LSTM ë ˆì´ì–´ í†µê³¼
        out, _ = self.lstm1(x)
        out = self.dropout1(out)
        out, _ = self.lstm2(out)
        # ë§ˆì§€ë§‰ ì‹œì (last time step)ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©
        out = self.dropout2(out[:, -1, :])
        out = torch.relu(self.fc1(out))
        # ì˜ˆì¸¡ê°’ ìƒì„± (1ê°œ ê°’ìœ¼ë¡œ ì¡°ì •í•˜ê¸° ìœ„í•´ ì„ í˜• ë³€í™˜ ì¶”ê°€)
        # Note: ìœ„ ì„¤ê³„ì—ì„œ fc2ë¥¼ fc1ì˜ ì¶œë ¥ 16ì— ë§ì¶° ìˆ˜ì •
        return nn.Linear(16, 1).to(device)(out)

# ìœ„ forward ë‚´ë¶€ì˜ Linear ë ˆì´ì–´ë¥¼ ìƒì„±ìì—ì„œ ì •ì˜í•˜ë„ë¡ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ ì„ ì–¸
class FinalStackedLSTM(nn.Module):
    def __init__(self, input_size):
        super(FinalStackedLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, 128, batch_first=True)
        self.dropout1 = nn.Dropout(0.2)
        self.lstm2 = nn.LSTM(128, 64, batch_first=True)
        self.dropout2 = nn.Dropout(0.1)
        self.fc1 = nn.Linear(64, 16)
        self.fc2 = nn.Linear(16, 1)

    def forward(self, x):
        out, _ = self.lstm1(x)
        out = self.dropout1(out)
        out, _ = self.lstm2(out)
        out = self.dropout2(out[:, -1, :])
        out = torch.relu(self.fc1(out))
        return self.fc2(out)

model = FinalStackedLSTM(input_size=len(features_list)).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. ëª¨ë¸ í•™ìŠµ
print(f"ğŸš€ {device}ì—ì„œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
model.train()
for epoch in range(50): 
    epoch_loss = 0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs.squeeze(), batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f"Epoch [{epoch+1}/50], Loss: {epoch_loss/len(train_loader):.6f}")

# 5. ì˜ˆì¸¡ ë° ì‹œê°í™”
model.eval()
with torch.no_grad():
    predictions = model(X_test).cpu().numpy()

# ì—­ìŠ¤ì¼€ì¼ë§ (ì „ë ¥ ì‚¬ìš©ëŸ‰ ë‹¨ìœ„ë¡œ ë³µêµ¬)
def inverse_scale(values, scaler, feature_count):
    dummy = np.zeros((len(values), feature_count))
    dummy[:, 1] = values.flatten() # Usageê°€ ì¸ë±ìŠ¤ 1ë²ˆ
    return scaler.inverse_transform(dummy)[:, 1]

pred_original = inverse_scale(predictions, scaler, len(features_list))
actual_original = inverse_scale(y_test.cpu().numpy(), scaler, len(features_list))

# ê²°ê³¼ ì¶œë ¥ (ìµœê·¼ 1ì£¼ì¼ì¹˜ 168ì‹œê°„ ì‹œê°í™”)
plt.figure(figsize=(15, 6))
plt.plot(actual_original[:168], label='ì‹¤ì œ ì „ë ¥ëŸ‰', color='#1f77b4', linewidth=2)
plt.plot(pred_original[:168], label='LSTM ì˜ˆì¸¡ê°’', color='#ff7f0e', linestyle='--', linewidth=2)
plt.title(f'{device} ê°€ì†: ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼ (ìµœê·¼ 168ì‹œê°„)')
plt.xlabel('ì‹œê°„(Hour)')
plt.ylabel('ì‚¬ìš©ëŸ‰(kW)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print("âœ¨ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì‹œê°í™” ì°½ì„ í™•ì¸í•˜ì„¸ìš”.")