import pandas as pd
import numpy as np
import koreanize_matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping

# ---------------------------------------------------------
# [ë‹¨ê³„ 0] í•˜ë“œì›¨ì–´ ë° í™˜ê²½ ì„¤ì •
# ---------------------------------------------------------

# 1. GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ (RTX 30 ì‹œë¦¬ì¦ˆ í•„ìˆ˜)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        
        # 2. í˜¼í•© ì •ë°€ë„(Mixed Precision) ì„¤ì •: FP16 ê°€ì† í™œì„±í™”
        policy = mixed_precision.Policy('mixed_float16')
        mixed_precision.set_global_policy(policy)
        print(f"âœ… í•˜ë“œì›¨ì–´ ê°€ì† í™œì„±í™”: {policy.name}")
    except RuntimeError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")


# [1] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# - ì´ì „ ë‹¨ê³„ì™€ ë™ì¼í•˜ê²Œ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ê³¼ ë‚ ì”¨ ë°ì´í„°ë¥¼ ê²°í•©í•˜ê³  ì£¼ë§ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
df_energy = pd.read_csv(r'.\data\household_daily_usage.csv', parse_dates=['dt'], index_col='dt')
df_weather = pd.read_csv(r'.\data\paris_weather_data.csv', parse_dates=['time'], index_col='time')

df_weather['temp_est'] = (df_weather['tmin'] + df_weather['tmax']) / 2
df_weather = df_weather[['temp_est', 'tmin', 'tmax', 'prcp']].interpolate().ffill().bfill()
df = df_energy.join(df_weather, how='inner')
df['is_weekend'] = df.index.dayofweek.map(lambda x: 1 if x >= 5 else 0)

# [2] í•™ìŠµ íŠ¹ì„±(Features) ì„ ì •
# - íƒ€ê²Ÿì¸ 'Global_active_power'ë¥¼ ì²« ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ë‘ì–´, ì´í›„ ì—­ìŠ¤ì¼€ì¼ë§(ì›ë˜ ê°’ ë³µì›) ì‹œ í¸ë¦¬í•˜ê²Œ êµ¬ì„±í•©ë‹ˆë‹¤.
features = ['Global_active_power', 'temp_est', 'tmin', 'tmax', 'prcp', 'is_weekend']
dataset = df[features].values

# [3] ë°ì´í„° ì •ê·œí™” (Scaling)
# - LSTMì€ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”ë˜ì—ˆì„ ë•Œ ìˆ˜ë ´ ì†ë„ì™€ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤.
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

# [4] ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± í•¨ìˆ˜ (Window Sliding Technique)
# - ê³¼ê±° ì¼ì£¼ì¼(seq_length)ì˜ ë°ì´í„°ë¥¼ ë³´ê³  ë‹¤ìŒ ë‚ ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” í˜•íƒœë¡œ ë°ì´í„°ì…‹ì„ ë³€í˜•í•©ë‹ˆë‹¤.
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        # ië¶€í„° i+seq_lengthê¹Œì§€ì˜ 6ê°œ ë³€ìˆ˜ ë°ì´í„°ë¥¼ ì…ë ¥(X)ìœ¼ë¡œ ì‚¬ìš©
        X.append(data[i:i+seq_length, :]) 
        # i+seq_length ì‹œì ì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼(ì „ë ¥ ì‚¬ìš©ëŸ‰)ì„ ì •ë‹µ(y)ìœ¼ë¡œ ì‚¬ìš©
        y.append(data[i+seq_length, 0])    
    return np.array(X), np.array(y)

seq_length = 7 # ê³¼ê±° 7ì¼ì¹˜ë¥¼ í•™ìŠµí•˜ì—¬ 8ì¼ì§¸ë¥¼ ì˜ˆì¸¡
X, y = create_sequences(scaled_data, seq_length)

# [5] í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„ë¦¬
# - ìˆœì„œê°€ ì¤‘ìš”í•œ ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ ëœë¤ ì…”í”Œë§ ì—†ì´ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë§ˆì§€ë§‰ 30ì¼ì„ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
train_size = len(X) - 30
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# [6] LSTM ì‹ ê²½ë§ ëª¨ë¸ ì„¤ê³„
# - LSTM ê³„ì¸µ: ì‹œê³„ì—´ì˜ ì¥ë‹¨ê¸° ê¸°ì–µì„ ë‹´ë‹¹
# - Dropout: ê³¼ì í•©(Overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ë¡œ ì¼ë¶€ ë‰´ëŸ°ì„ ë”
# - Dense(1): ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒ ë‚ ì˜ ì „ë ¥ëŸ‰ 1ê°œë¥¼ ì˜ˆì¸¡
model = Sequential([
    # ì…ë ¥ ê·œê²© ì •ì˜: (7ì¼ì˜ íƒ€ì„ìŠ¤í…, 6ê°œì˜ íŠ¹ì„±)
    Input(shape=(X_train.shape[1], X_train.shape[2])),
    
    # ì²« ë²ˆì§¸ LSTM ê³„ì¸µ: return_sequences=TrueëŠ” ë‹¤ìŒ LSTM ì¸µìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•¨
    LSTM(128, return_sequences=True),

    # ë‘ ë²ˆì§¸ LSTM ê³„ì¸µ: return_sequences=FalseëŠ” ì‹œí€€ìŠ¤ë¥¼ ìš”ì•½í•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“¦
    LSTM(64, return_sequences=False),
    
    # ì¶œë ¥ ê³„ì¸µ: ë‹¤ìŒ ë‚ ì˜ ì „ë ¥ ì‚¬ìš©ëŸ‰ 1ê°œ ì˜ˆì¸¡
    Dense(1) 
])

# [7] ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ
model.compile(optimizer='adam', loss='mse')

# EarlyStopping ì„¤ì •: ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µêµ¬í•˜ë„ë¡ restore_best_weights ì¶”ê°€ ê¶Œì¥
early_stop = EarlyStopping(
    monitor='val_loss', 
    patience=10, 
    restore_best_weights=True
)

print("ğŸš€ ë‹¤ë³€ëŸ‰ LSTM í•™ìŠµ ì‹œì‘...")
history = model.fit(
    X_train, y_train, 
    epochs=50, 
    batch_size=16, 
    validation_split=0.1, 
    callbacks=[early_stop], 
    verbose=1
)
# [8] ì˜ˆì¸¡ ë° ì—­ì •ê·œí™” (Inversing Scaling)
# - ëª¨ë¸ì€ 0~1 ì‚¬ì´ ê°’ì„ ì¶œë ¥í•˜ë¯€ë¡œ, ì´ë¥¼ ì‹¤ì œ ë‹¨ìœ„ì¸ kWë¡œ ë³µêµ¬í•´ì•¼ í•©ë‹ˆë‹¤.
predictions = model.predict(X_test)

# - ì—­ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰í•˜ë ¤ë©´ í•™ìŠµ ë‹¹ì‹œ ì‚¬ìš©í•œ 6ê°œ ì»¬ëŸ¼ì˜ í˜•ì‹ì„ ë§ì¶°ì•¼ í•¨ (ë”ë¯¸ í–‰ë ¬ í™œìš©)
predict_copies = np.zeros((len(predictions), len(features)))
predict_copies[:, 0] = predictions.flatten() # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì— ì˜ˆì¸¡ê°’ ë°°ì¹˜
inv_predictions = scaler.inverse_transform(predict_copies)[:, 0] # ì›ë˜ ë‹¨ìœ„ë¡œ ë³µì›

# - ì‹¤ì œ ê°’(y_test)ë„ ë¹„êµë¥¼ ìœ„í•´ ë™ì¼í•˜ê²Œ ì—­ìŠ¤ì¼€ì¼ë§ ì§„í–‰
actual_copies = np.zeros((len(y_test), len(features)))
actual_copies[:, 0] = y_test
inv_actual = scaler.inverse_transform(actual_copies)[:, 0]

# [9] ìµœì¢… ê²°ê³¼ ì‹œê°í™” ë° ë¹„êµ
plt.figure(figsize=(12, 6))
plt.plot(df.index[-30:], inv_actual, label='Actual (ì‹¤ì œ)', color='blue', marker='o')
plt.plot(df.index[-30:], inv_predictions, label='LSTM (ì¶”ë¡ )', color='orange', linestyle='--', marker='s')
plt.title('ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ì¶”ë¡  (LSTM)')
plt.ylabel('Global Active Power (kW)')
plt.legend()
plt.grid(True)
plt.show()