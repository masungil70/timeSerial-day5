import pandas as pd
import numpy as np
import koreanize_matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import joblib
import os
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision

# 1. í•˜ë“œì›¨ì–´ ê°€ì† ì„¤ì •
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        
        # RTX 30 ì‹œë¦¬ì¦ˆ ì´ìƒ í•„ìˆ˜: í˜¼í•© ì •ë°€ë„ í™œì„±í™”
        policy = mixed_precision.Policy('mixed_float16')
        mixed_precision.set_global_policy(policy)
        print(f"ğŸš€ ê°€ì† ì •ì±… ì ìš©: {policy.name}")
    except RuntimeError as e:
        print(e)


# [ë‹¨ê³„ 1 & 2] ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ê³µí•™ (Cycle Encoding)
df = pd.read_csv('./data/power_usage_dataset_3month.csv')
df['Date'] = pd.to_datetime(df['Date'])

df['hour'] = df['Date'].dt.hour
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 23)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 23)

df['weekday'] = df['Date'].dt.weekday
df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 6)
df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 6)

features_list = ['Temperature', 'Usage', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos']
data = df[features_list].values

# [ë‹¨ê³„ 3] ë°ì´í„° ì „ì²˜ë¦¬
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

def create_sequences(data, window_size=168):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i + window_size, :]) 
        y.append(data[i + window_size, 1]) 
    return np.array(X), np.array(y)

window_size = 168 
X, y = create_sequences(scaled_data, window_size)

split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# [ë‹¨ê³„ 4] ëª¨ë¸ ì„¤ê³„
model = Sequential([
    Input(shape=(X_train.shape[1], X_train.shape[2])),
    
    LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)),
    Dropout(0.2),
    
    LSTM(64, activation='tanh', return_sequences=False, kernel_regularizer=l2(0.0001)),
    Dropout(0.1),
    
    # Mixed Precision ëŒ€ì‘: ìµœì¢… ì¶œë ¥ì¸µì€ float32
    Dense(1, dtype='float32')
])

# [ë‹¨ê³„ 5] ì»´íŒŒì¼ ë° í•™ìŠµ (XLA ì»´íŒŒì¼ í™œì„±í™”)
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse', jit_compile=True)

early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=256,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=1
)

# [ë‹¨ê³„ 6] ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
# í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±í•˜ëŠ” ë¡œì§ ì¶”ê°€
save_dir = './model'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# 1) ëª¨ë¸ ì €ì¥: ìµœì‹  Keras ë°©ì‹ì¸ .keras í™•ì¥ì ê¶Œì¥ (ë˜ëŠ” .h5)
model_path = os.path.join(save_dir, 'power_usage_lstm_model.h5')
model.save(model_path)

# 2) ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: joblib ì‚¬ìš©
scaler_path = os.path.join(save_dir, 'power_usage_scaler.pkl')
joblib.dump(scaler, scaler_path)

print(f"\nâœ… ì €ì¥ ì™„ë£Œ:")
print(f"   - ëª¨ë¸: {model_path} ({os.path.getsize(model_path)/(1024*1024):.2f} MB)")
print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")