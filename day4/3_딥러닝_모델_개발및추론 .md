# 딥러닝 모델

1. 딥러닝 모델의 개발(Training)
2. 딥러닝 모델의 실전 활용(Inference)

---

## 1. 딥러닝 모델의 개발(Training)

스마트 전력량 데이터를 활용해 딥러닝 모델을 만드는 과정은 **'시계열 데이터 예측(Time-Series Forecasting)'** 의 전형적인 사례입니다. 전력 소모량은 시간의 흐름에 따라 패턴(계절성, 주기성)이 뚜렷하기 때문이죠.

day1에 학습한 딥러닝 개발 5단계를 스마트 전력량 데이터에 맞춰 다시 구현해 보겠습니다.

---

### 1단계: 데이터 수집 및 전처리 (Data Preparation)

스마트 미터기에서 수집된 원시(Raw) 데이터는 바로 모델에 넣을 수 없습니다. 모델이 학습하기 좋은 형태로 가공합니다.

* **데이터 로드:** CSV 파일을 읽어오고 날짜 형식을 변환합니다.
* **결측치, 이상치 처리:**  $0 \sim 3.0$  범위를 벗어나는 값을 `NaN`으로 바꾸고 선형 보간(`interpolate`)합니다.
* **특성 공학(Feature Engineering):** 시간과 요일의 주기성을 반영하기 위해 `sin/cos` 변환을 적용했습니다. 이는 단순 숫자가 아닌 '순환하는 시간의 개념'을 모델에게 알려주기 위함입니다.
* **정규화(Scaling):** 딥러닝은 데이터 수치가 크면 학습이 불안정합니다. `MinMaxScaler`를 사용하여 전력량을 $0$과 $1$ 사이로 변환합니다.
* **시퀀스 생성:** LSTM이 학습할 수 있도록 **과거 1주일(168시간)** 의 데이터를 묶어 하나의 학습 단위($X$)로 만들었습니다.

---

### 2단계: 모델 설계 (Model Architecture)

모델의 뼈대를 만드는 단계입니다.
전력량 데이터의 '시간적 흐름'을 파악하기 위해 **LSTM** 구조를 설계합니다.
신경망의 구조를 정의하고 성능 향상을 위한 기법을 적용하는 단계입니다

* **Stacked LSTM:** LSTM 레이어를 두 층으로 쌓아 복잡한 패턴을 학습하게 했습니다.
* **과적합 방지 도구:** `Dropout`을 통해 특정 뉴런에 의존하지 않게 하고, `L2 규제`를 더해 가중치가 너무 커지지 않도록 제어했습니다.
* **최적화 설정:** GPU 가속을 위한 **XLA 컴파일(`jit_compile=True`)** 과 메모리 효율을 위한 **혼합 정밀도(`mixed_float16`)** 를 설정했습니다.

---

### 3단계: 모델 컴파일 및 학습 (Compile & Training)

설계한 모델을 실제 데이터로 훈련시키는 단계입니다.

* **컴파일:** `Adam` 옵티마이저와 `MSE` 손실 함수를 결합했습니다.
* **학습 실행:** `batch_size=256`으로 설정하여 GPU 연산 효율을 극대화했습니다.
* **조기 종료(`EarlyStopping`):** 검증 오차(`val_loss`)가 더 이상 줄어들지 않으면 학습을 멈춰 최적의 상태를 보존합니다.

---

### 4단계: 모델 평가 및 튜닝 (Evaluation & Tuning)

학습된 모델의 성능을 검증하는 단계입니다.
학습에 쓰지 않은 '테스트 데이터'로 실력을 검증합니다.

* **예측 및 역스케일링:** 모델이 내뱉은  사이의 값을 `inverse_transform`을 통해 실제 전력 단위인 `kW`로 복원했습니다.
* **시각화:** 그래프를 통해 실제값과 예측값의 추이를 비교했습니다.
* **성능 지표:** `history` 객체를 통해 학습 곡선을 확인하고 파라미터를 수정할 근거를 얻습니다.

---

### 5단계: 배포 및 모니터링 (Deployment & Monitoring)

완성된 모델을 나중에 사용할 수 있도록 파일로 저장하는 단계입니다.

* **모델 저장:** 학습된 두뇌 정보가 담긴 `.h5` 파일을 저장합니다.
* **자원 저장:** 예측 시 똑같은 기준으로 데이터를 변환해야 하므로 `scaler.pkl` 파일을 함께 저장합니다.

---

#### Training 코드

파일명 : day4/step3/training.py

```python

import pandas as pd
import numpy as np
import koreanize_matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import joblib
import os
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision

# ---------------------------------------------------------
# [단계 0] 하드웨어 및 환경 설정
# ---------------------------------------------------------

# 1. GPU 메모리 동적 할당 (RTX 30 시리즈 필수)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        
        # 2. 혼합 정밀도(Mixed Precision) 설정: FP16 가속 활성화
        policy = mixed_precision.Policy('mixed_float16')
        mixed_precision.set_global_policy(policy)
        print(f"✅ 하드웨어 가속 활성화: {policy.name}")
    except RuntimeError as e:
        print(f"❌ 설정 오류: {e}")

# ---------------------------------------------------------
# [데이터 전처리 : 단계 1] 데이터 로드 및 특성 공학
# ---------------------------------------------------------
df = pd.read_csv('./data/power_usage_dataset_3month.csv')
df['Date'] = pd.to_datetime(df['Date'])

# [데이터 전처리 : 단계 2] 이상치 탐지 및 제거
# 0 ~ 3.0 초과인 값을 찾아 NaN(결측치)으로 바꿉니다.
# 'Usage' 컬럼을 기준으로 처리하며, 데이터 특성에 따라 컬럼명을 확인해 주세요.
df.loc[(df['Usage'] < 0) | (df['Usage'] > 3.0), 'Usage'] = np.nan

# [데이터 전처리 : 단계 3] 선형 보간(Linear Interpolation) 수행
# NaN 앞뒤의 데이터를 연결하는 선을 그려 중간값을 채웁니다. 
# 시계열 데이터의 흐름을 깨지 않는 가장 표준적인 방법입니다.
df['Usage'] = df['Usage'].interpolate(method='linear')

# [데이터 전처리 : 단계 4] 잔여 결측치 처리
# 만약 데이터의 맨 첫 줄이나 맨 마지막 줄이 NaN이라면 보간되지 않을 수 있습니다.
# 이런 경우 근처의 값으로 채워(ffill, bfill) 완벽하게 결측치를 없앱니다.
df['Usage'] = df['Usage'].ffill().bfill()

# [데이터 전처리 : 단계 5] 특성 공학 (Feature Engineering)
# 시간 및 요일 주기성 반영 (Cyclic Encoding)
df['hour'] = df['Date'].dt.hour
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 23)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 23)

df['weekday'] = df['Date'].dt.weekday
df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 6)
df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 6)

# 분석 필드 (총 6개)
features_list = ['Temperature', 'Usage', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos']
data = df[features_list].values

# ---------------------------------------------------------
# [데이터 전처리 : 단계 6] 데이터 스케일링
# ---------------------------------------------------------
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

def create_sequences(data, window_size=168):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i + window_size, :]) 
        y.append(data[i + window_size, 1]) # Target: Usage
    return np.array(X), np.array(y)

window_size = 168 # 1주일 패턴 학습
X, y = create_sequences(scaled_data, window_size)

# 데이터 분할 (8:2)
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ---------------------------------------------------------
# [단계 2 : 모델 설계] 최신 Keras 스타일 모델 설계 (Input 레이어 명시)
# ---------------------------------------------------------
model = Sequential([
    # 명시적 입력 정의: (타임스텝, 피처수)
    Input(shape=(X_train.shape[1], X_train.shape[2])),
    
    # 첫 번째 LSTM 계층: L2 규제 및 드롭아웃
    LSTM(128
         , activation='tanh'
         , return_sequences=True
         , kernel_regularizer=l2(0.0001)),
    Dropout(0.2),
    
    # 두 번째 LSTM 계층
    LSTM(64
         , activation='tanh'
         , return_sequences=False
         , kernel_regularizer=l2(0.0001)),
    Dropout(0.1),
    
    # 출력 계층: 혼합 정밀도 대응을 위해 float32 명시
    Dense(1, dtype='float32')
])

# ---------------------------------------------------------
# [단계 3] 컴파일 및 학습 (XLA 적용)
# ---------------------------------------------------------
optimizer = Adam(learning_rate=0.001)

# jit_compile=True: GPU 하드웨어 가속 최적화 (XLA 컴파일러)
model.compile(optimizer=optimizer, loss='mse', jit_compile=True)

early_stop = EarlyStopping(
    monitor='val_loss',         # 감시 대상: 검증 데이터의 손실 값
    patience=7,                 # 성능 개선이 없을 때 기다려줄 에포크 횟수
    restore_best_weights=True   # 학습 종료 후 가장 성적이 좋았던 시점의 가중치로 복원
)

print("\n🚀 모델 학습을 시작합니다...")
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=256, # 대량 데이터 및 GPU 가속을 위한 큰 배치 사이즈
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=1
)

# ---------------------------------------------------------
# [단계 4] 예측 및 역스케일링
# ---------------------------------------------------------
predictions_scaled = model.predict(X_test)

def get_original_units(scaled_values, scaler, feature_count, target_idx=1):
    dummy = np.zeros((len(scaled_values), feature_count))
    dummy[:, target_idx] = scaled_values.flatten()
    return scaler.inverse_transform(dummy)[:, target_idx]

y_test_original = get_original_units(y_test, scaler, len(features_list))
predictions_original = get_original_units(predictions_scaled, scaler, len(features_list))

# ---------------------------------------------------------
# [단계 5] 시각화 및 저장
# ---------------------------------------------------------
# 1. 예측 결과 시각화
plt.figure(figsize=(14, 6))
plt.plot(y_test_original[:168], label='실제값', color='#1f77b4', linewidth=2)
plt.plot(predictions_original[:168], label='예측값', color='#ff7f0e', linestyle='--', linewidth=2)
plt.title('최적화된 Stacked LSTM: 전력 사용량 예측 결과 (1주일)')
plt.xlabel('시간')
plt.ylabel('전력 사용량(kW)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 2. 모델 및 스케일러 저장
save_dir = './model'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

model_path = os.path.join(save_dir, 'power_usage_lstm_model.keras')
scaler_path = os.path.join(save_dir, 'power_usage_scaler.pkl')

model.save(model_path)
joblib.dump(scaler, scaler_path)

print(f"\n✅ 완료: 모델({model_path}) 및 스케일러({scaler_path}) 저장 성공!")

```

---

## 2. 딥러닝 모델의 실전 활용(Inference)

딥러닝 모델을 개발하는 것(Training)만큼이나 중요한 것이 바로 모델을 실제 서비스에 적용하는 **추론(Inference)** 과정입니다. 학습이 '공부'라면, 추론은 학습한 내용을 바탕으로 '시험'을 치르거나 '문제'를 해결하는 실전 단계라고 할 수 있죠.

인퍼런스 단계는 **불러오기 → 데이터 가공 → 추론 → 결과 활용** 프로세스로 진행을 합니다.

1. 자원 로드 및 환경 설정 (Resource Loading & Setting Assets)
2. 데이터 추출 및 특성 공학 (Data Preprocessing & Validation)
3. 정규화 및 추론 (Inference)
4. 결과 복원 및 결과 활용 (Post-processing & Delivery)

---

### 인퍼런스 상세 프로세스

#### 1. 자원 로드 및 환경 설정 (Resource Loading & Setting Assets)

단순히 모델만 불러오는 것이 아니라, 모델이 세상을 해석하던 **'기준'** 을 모두 가져와야 합니다.
모델이 예측을 시작할 수 있도록 학습된 "두뇌"와 데이터의 "기준점"을 메모리에 올리는 단계입니다.

* **모델 로드 (`load_model`):** 학습이 완료된 `.h5` 파일을 불러옵니다. 이때 `compile=False` 옵션을 주어 예측에 불필요한 학습용 설정(Optimizer 등)을 생략하고 속도를 높였습니다.
* **스케일러 로드 (`joblib.load`):** 학습 시 데이터 수치를 로 압축했던 기준(`scaler.pkl`)을 가져옵니다. 이 기준이 없으면 모델의 출력을 사람이 이해하는 단위(kW)로 바꿀 수 없습니다.
* **최적화 설정:** `mixed_float16` 정책을 통해 GPU 연산 속도를 최적화했습니다.

```python
# compile 복원
model = load_model('model.h5')

# compile 비복원(가중치만 복원)
model = load_model('model.h5', compile=False)
```

모델을 불러오는 두 코드의 결정적인 차이는 **"학습에 필요한 설정값(컴파일 정보)까지 복원할 것인가, 아니면 모델의 구조와 무게(가중치)만 가져올 것인가"** 에 있습니다.

1. `compile=True` (기본값: 생략시)

`load_model('model.h5')`

이 방식은 모델의 **모든 것**을 통째로 가져옵니다.

* **복원 내용:** 모델 구조(Layer), 학습된 가중치(Weights)뿐만 아니라 **옵티마이저(Optimizer), 손실 함수(Loss), 평가지표(Metrics)** 상태까지 모두 복원합니다.
* **장점:** `model.fit()`을 호출하여 **이전 학습 상태 그대로 이어서 학습(Resume Training)** 할 수 있습니다. 옵티마이저의 상태(예: Adam의 현재 학습률 등)가 유지되기 때문입니다.
* **단점:** 모델을 저장할 때 사용했던 **커스텀 레이어(예: 앞서 만든 AttentionLayer)** 나 특수한 손실 함수가 있다면, 불러올 때 해당 클래스를 알려주지 않으면 에러가 발생할 확률이 높습니다.

2. `compile=False` (두번째 코드)

`load_model('model.h5', compile=False)`

이 방식은 모델의 **겉모양과 알맹이(가중치)** 만 가져오고, 학습 도구들은 버립니다. 추론만 할 경우 많이 사용하는 방법입니다.

* **복원 내용:** 모델의 층 구조와 학습된 가중치만 가져옵니다. 옵티마이저나 손실 함수 정보는 불러오지 않습니다.
* **장점:**
    1. 단순히 **추론(Prediction)만 할 목적** 일 때 메모리를 아끼고 로딩 속도를 높일 수 있습니다.
    2. 커스텀 레이어가 포함된 모델을 불러올 때, 컴파일 단계에서 발생하는 복잡한 의존성 에러를 피할 수 있습니다.
* **단점:** 이 상태로는 바로 `model.fit()`을 쓸 수 없습니다. 다시 학습시키려면 반드시 `model.compile()`을 직접 호출하여 새로운 옵티마이저를 설정해줘야 합니다.

---

| 비교 항목 | `compile=True` (기본) | `compile=False` |
| --- | --- | --- |
| **주요 용도** | **추가 학습** 이 필요할 때 | **단순 예측(추론)** 만 할 때 |
| **복원 범위** | 구조 + 가중치 + 옵티마이저 등 | 구조 + 가중치 |
| **에러 발생 확률** | 상대적으로 높음 (커스텀 요소 시) | 낮음 (가벼운 로딩) |
| **추가 작업** | 즉시 학습/예측 가능 | 학습하려면 다시 `compile()` 필요 |

---

#### 2. 데이터 추출 및 특성 공학 (Data Preprocessing & Validation)

실시간(또는 특정 시점) 데이터를 모델이 배운 것과 똑같은 형태로 가공하는 단계입니다.
단순 가공을 넘어 **'검증'** 단계가 추가될 수 있습니다.

* **시계열 필터링:** 예측하려는 시점(`target_time`)을 기준으로 **과거 24시간** 의 데이터를 데이터셋에서 찾아냅니다.
* **주기성 반영 (Feature Engineering):** 시간과 요일을 단순히 숫자로 넣지 않고, `sin`/`cos` 함수를 이용해 **23시와 0시가 연결되는 순환 구조** 로 변환하여 모델이 시간의 흐름을 이해하게 돕습니다.

### 3. 정규화 및 추론 (Inference)

가공된 데이터를 모델에 주입하여 결과를 뽑아내는 핵심 단계입니다.

* **입력 데이터 정규화 (`scaler.transform`):** 수집된 온도, 전력량 등의 실제 데이터를 모델이 이해할 수 있는  사이 값으로 변환합니다.
* **텐서 변환 (Reshaping):** LSTM 모델의 입력 규격인 **(배치 크기, 타임스텝, 특성 수)** 즉, `(1, 24, 6)` 형태로 데이터를 재구성합니다.
* **추론 (`model.predict`):** 모델에 데이터를 입력하여 1시간 뒤의 전력 사용량을 예측합니다. 결과값은 여전히 0 ~ 1 사이의 압축된 값입니다.

### 4. 결과 복원 및 결과 활용 (Post-processing & Delivery)

모델의 답변을 사람이 읽을 수 있는 유의미한 정보로 변환하는 단계입니다.

* **더미 행렬(Dummy Matrix) 생성:** 학습 시 사용한 스케일러가 6개의 특성을 가졌으므로, 예측값 하나를 복원하기 위해 동일한 6개 열을 가진 행렬을 만듭니다.
* **역스케일링 (`inverse_transform`):** 압축되었던 값을 다시 실제 단위인 `kW`로 되돌립니다.
* **최종 활용:** 예측된 전력량을 출력하여 사용자에게 정보를 제공합니다.

---

파일명 : day4/step4/inference.py

## 전력량 예측 프로그램

```python
import pandas as pd
import numpy as np
import joblib
from tensorflow.keras.models import load_model
from tensorflow.keras import mixed_precision

# 혼합 정밀도(mixed_float16) 정책 설정
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)


# 1. 저장된 자원 로드 (불러오기)
# 컴파일 설정을 불러오지 않음
# 학습 시에는 '정답과 얼마나 틀렸는지' 계산하는 MSE(Loss) 정보가 필수적이지만, 예측 시에는 '입력을 넣고 출력만 뽑는' 연산만 수행하기 때문입니다.
model = load_model('./model/power_usage_lstm_model.h5', compile=False)
scaler = joblib.load('./model/power_usage_scaler.pkl')

# 2. 데이터 준비 (데이터 가공)
# 실제 데이터셋에서 2025-09-20 01:00 ~ 2025-09-21 00:00 (24시간) 데이터를 추출합니다.
df = pd.read_csv('./data/power_usage_dataset_3month.csv')
df['Date'] = pd.to_datetime(df['Date'])

# 예측 기준 시간 설정
target_time = pd.to_datetime('2025-09-21 01:00')
start_time = target_time - pd.Timedelta(hours=24)
end_time = target_time - pd.Timedelta(hours=1)

# 과거 24시간 데이터 필터링
past_24h = df[(df['Date'] >= start_time) & (df['Date'] <= end_time)].copy()

# [단계 3] 특성 공학: 시간 및 요일 주기성 반영
past_24h['hour'] = past_24h['Date'].dt.hour
past_24h['hour_sin'] = np.sin(2 * np.pi * past_24h['hour'] / 23)
past_24h['hour_cos'] = np.cos(2 * np.pi * past_24h['hour'] / 23)
past_24h['weekday'] = past_24h['Date'].dt.weekday
past_24h['weekday_sin'] = np.sin(2 * np.pi * past_24h['weekday'] / 6)
past_24h['weekday_cos'] = np.cos(2 * np.pi * past_24h['weekday'] / 6)


# 과거 데이터에 시간 특성 추가
input_features = past_24h[['Temperature', 'Usage', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos']].values

# 4. 정규화 및 텐서 변환
# 학습 때 사용한 스케일러로 데이터를 0~1 사이로 변환합니다.
scaled_input = scaler.transform(input_features)
# 모델 입력 모양에 맞게 변환: (Batch, Time, Features) -> (1, 24, 6)
X_input = scaled_input.reshape(1, 24, 6)

# 5. 예측 (Inference)
pred_scaled = model.predict(X_input, verbose=0)

# 6. 결과 복원 (결과 활용)
# 예측된 0~1 사이 값을 실제 전력량(kW) 단위로 되돌립니다.
# 21일 01시의 온도(19.5도라 가정)와 함께 역스케일링을 수행합니다.
target_temp = 19.5  # 21일 01시 온도
target_hour_sin = np.sin(2 * np.pi * target_time.hour / 23)
target_hour_cos = np.cos(2 * np.pi * target_time.hour / 23)
target_weekday_sin = np.sin(2 * np.pi * target_time.weekday() / 6)
target_weekday_cos = np.cos(2 * np.pi * target_time.weekday() / 6)

# 역스케일링을 위해 더미 행렬 생성 (4개 특성 규격을 맞춤)
dummy = np.zeros((1, 6))
dummy[0, 0] = target_temp      # 온도
dummy[0, 1] = pred_scaled[0,0] # 예측된 전력량
dummy[0, 2] = target_hour_sin  # 시간 sin
dummy[0, 3] = target_hour_cos  # 시간 cos
dummy[0, 4] = target_weekday_sin  # 요일 sin
dummy[0, 5] = target_weekday_cos  # 요일 cos

# 예측된 전력량 역스케일링해서 얻기 
final_prediction = scaler.inverse_transform(dummy)[0, 1]

print("-" * 50)
print(f"📅 예측 대상 시간: {target_time}")
print(f"🌡️ 입력된 기온: {target_temp}°C")
print(f"⚡ 예측된 전력 사용량: {final_prediction:.4f} kW")
print("-" * 50)
```

---

### 주의할 점: "학습(Train)과 추론(Inference)의 차이"

데이터 가공 단계에서 가장 많이 하시는 실수가 **추론 시점에 스케일러를 새로 `fit` 시키는 것**입니다. 반드시 학습 때 저장해둔 스케일러의 기준(`transform`)만 사용해야 한다는 점을 꼭 기억하세요!

---

## 추론시 CPU vs GPU

서비스의 성격과 모델의 규모에 따라 다르지만, 많은 실전 사례에서 CPU 추론만으로도 충분히 훌륭한 성능내고 있습니다.
무조건 GPU가 좋은 것은 아니며, 오히려 비용과 운영 효율 면에서는 CPU가 유리한 경우가 많습니다.

---

## 1. CPU 추론이 유리한 경우 (충분한 경우)

많은 정형 데이터(Tabular Data) 기반의 딥러닝 모델이나 가벼운 모델들은 CPU에서도 밀리초($ms$) 단위로 결과를 내놓습니다.

* **모델이 가벼울 때:** 레이어 수가 적은 MLP, 가벼운 CNN(MobileNet 등)은 CPU에서도 충분히 빠릅니다.
* **비용 효율성:** GPU 서버는 CPU 서버보다 최소 수 배 이상 비쌉니다. 트래픽이 아주 많지 않다면 CPU 서버 여러 대를 두는 것이 경제적입니다.
* **지연 시간(Latency)보다 처리량(Throughput)이 중요할 때:** 실시간 응답이 0.1초 이내여야 하는 서비스가 아니라면 CPU로도 충분합니다.
* **복잡한 전처리:** 인퍼런스 과정에서 데이터 가공(Preprocessing) 비중이 높다면, 어차피 CPU가 일을 해야 하므로 GPU와의 데이터 전송 시간(Overhead) 때문에 오히려 CPU 단독 처리가 빠를 수도 있습니다.

---

## 2. GPU 추론이 반드시 필요한 경우

다음과 같은 경우에는 CPU만 사용하면 서비스가 매우 느려지거나 서버가 마비될 수 있습니다.

* **거대 모델 (LLM 등):** GPT, Llama 같은 대규모 언어 모델이나 고해상도 이미지 생성 모델(Stable Diffusion)은 CPU에서 구동 시 응답 하나에 수십 초 이상 걸릴 수 있습니다.
* **실시간 고화질 영상 처리:** CCTV 분석이나 자율주행처럼 초당 수십 프레임의 이미지를 실시간으로 분석해야 할 때는 GPU의 병렬 연산이 필수적입니다.
* **동시 접속자가 매우 많을 때:** 수천 명의 요청을 동시에 처리해야 하는 경우, GPU 한 장이 CPU 수십 대의 역할을 대신할 수 있습니다.

---

## 3. CPU 성능을 극대화하는 방법 (Optimization)

CPU만 사용하는 경우 그냥 돌리기보다는 아래와 같은 최적화를 거치는 것이 실전 프로세스의 핵심입니다.

1. **OpenVINO (Intel) / ONNX Runtime:** Intel CPU를 사용한다면 OpenVINO 라이브러리를 통해 연산 속도를 2~10배까지 끌어올릴 수 있습니다.
2. **양자화 (Quantization):** 모델의 가중치를 **32-bit (FP32)** 에서 **8-bit (INT8)** 로 줄이면 메모리 사용량은 1/4로 줄고, CPU 연산 속도는 비약적으로 빨라집니다.
3. **멀티쓰레딩 활용:** CPU의 여러 코어를 동시에 사용하여 배치를 처리하도록 설정합니다.

---

### CPU vs GPU 선택 가이드

| 구분 | CPU 추론 | GPU 추론 |
| --- | --- | --- |
| **초기 비용** | 저렴함 (일반 서버 활용) | 비쌈 (고가의 장비/클라우드) |
| **운영 난이도** | 낮음 (라이브러리 설치 간편) | 높음 (CUDA, 드라이버 버전 관리) |
| **적합한 모델** | 정형 데이터 예측, 가벼운 모델 | 생성형 AI, 고해상도 영상, 복잡한 모델 |
| **추천 환경** | 일반 웹 서비스 API, IoT 기기 | 실시간 영상 분석, 대규모 AI 서비스 |

---