import numpy as np
import pandas as pd
import koreanize_matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import  Layer, Input, LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
import os
import joblib

# 0. GPU ë° í˜¼í•© ì •ë°€ë„ ì„¤ì • (ì„ íƒ ì‚¬í•­)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
data_df = pd.read_csv('./data/flights.csv')
passengers = data_df['Passengers'].values.astype(float)

# ê³„ì ˆì„± ì°¨ë¶„ (Seasonal Differencing: í˜„ì¬ - 12ê°œì›” ì „)
seasonal_period = 12
diff_passengers = passengers[seasonal_period:] - passengers[:-seasonal_period]
diff_passengers = diff_passengers.reshape(-1, 1)

# ë°ì´í„° ì •ê·œí™”
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(diff_passengers)

# ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ê²½ë¡œ í™•ì¸
save_dir = './model'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
joblib.dump(scaler, os.path.join(save_dir, "air_passengers_scaler.pkl"))

# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_dataset(dataset, look_back=12):
    X, y = [], []
    for i in range(len(dataset) - look_back):
        X.append(dataset[i:(i + look_back), 0])
        y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(y)

look_back = 12
X, y = create_dataset(data_scaled, look_back)
X = X.reshape((X.shape[0], X.shape[1], 1))

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
train_size = len(X) - 24
X_train, X_test = X[:train_size], X[train_size:] # ë˜ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì¸ë±ì‹±
y_train, y_test = y[:train_size], y[train_size:]

# 2. Attention ë ˆì´ì–´ ì •ì˜ (Serialization ëŒ€ì‘ ìµœì í™”)
@tf.keras.utils.register_keras_serializable() # ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì‹œ ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì¸ì‹ì„ ìœ„í•œ ë°ì½”ë ˆì´í„°
class AttentionLayer(Layer):
    """
    LSTMì˜ ì¶œë ¥ ì‹œí€€ìŠ¤ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘(Attention)í•˜ì—¬ 
    ê°€ì¤‘ í•©ì‚°ëœ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ì»¤ìŠ¤í…€ ë ˆì´ì–´ì…ë‹ˆë‹¤.
    """
    def __init__(self, **kwargs):
        # ë¶€ëª¨ í´ë˜ìŠ¤(layers.Layer)ì˜ ì´ˆê¸°í™” ë£¨í‹´ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        """
        ë ˆì´ì–´ê°€ ì²˜ìŒ í˜¸ì¶œë  ë•Œ ì‹¤í–‰ë˜ë©°, í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜(Weight)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        input_shape:     (Batch_size, Time_steps, Input_dim) í˜•íƒœì…ë‹ˆë‹¤.
        ë°°ì—´ì„ ì™¼ìª½ì— ì ‘ê·¼   :  0           1           2
        ë°°ì—´ì„ ì™¼ë¥¸ìª½ì— ì ‘ê·¼ : -3          -2          -1
        """
        # 1. í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ W ì •ì˜: ê° ì‹œì ì˜ íŠ¹ì§•ê°’ì— ê³±í•´ì§ˆ ê°€ì¤‘ì¹˜ í–‰ë ¬
        # í˜•íƒœ: (ì…ë ¥ ì°¨ì›, 1) -> ê° ì‹œì ì˜ ë²¡í„°ë¥¼ ìŠ¤ì¹¼ë¼ ì ìˆ˜ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•¨
        self.W = self.add_weight(name="att_weight", 
                                 shape=(input_shape[-1], 1), 
                                 initializer="normal",
                                 trainable=True)
        
        # 2. í¸í–¥ b ì •ì˜: í™œì„±í™” í•¨ìˆ˜ ì ìš© ì „ ë”í•´ì§€ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ìƒìˆ˜
        # í˜•íƒœ: (íƒ€ì„ìŠ¤í… ìˆ˜, 1) -> ê° ì‹œì ë³„ë¡œ ê³ ìœ í•œ í¸í–¥ê°’ ë¶€ì—¬
        self.b = self.add_weight(name="att_bias", 
                                 shape=(input_shape[1], 1), 
                                 initializer="zeros",
                                 trainable=True)
        
        # ê°€ì¤‘ì¹˜ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŒì„ ì„ ì–¸í•©ë‹ˆë‹¤.
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        """
        ì‹¤ì œ ì—°ì‚°ì´ ì¼ì–´ë‚˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (Forward Propagation)
        inputs: LSTMì˜ ì¶œë ¥ê°’ (Batch, Time_steps, Feature_dim)
        """
        # [ë‹¨ê³„ 1] ì ìˆ˜ ê³„ì‚° (Score Calculation)
        # inputs(W) + b ë¥¼ í†µí•´ ê° ì‹œì ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 'ì—ë„ˆì§€ ì ìˆ˜'ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        # tanh í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ë¥¼ -1ê³¼ 1 ì‚¬ì´ë¡œ ë¹„ì„ í˜• ë³€í™˜í•©ë‹ˆë‹¤.
        et = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)

        # [ë‹¨ê³„ 2] í™•ë¥  ë³€í™˜ (Attention Weights)
        # Softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì‹œì ì˜ et í•©ê³„ê°€ 1(100%)ì´ ë˜ë„ë¡ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        # axis=1ì€ íƒ€ì„ìŠ¤í… ë°©í–¥ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
        at = tf.nn.softmax(et, axis=1)

        # [ë‹¨ê³„ 3] ê°€ì¤‘ì¹˜ ì ìš© (Weight Application)
        # ì›ë³¸ ì…ë ¥ê°’(inputs)ì— ê³„ì‚°ëœ í™•ë¥  ê°€ì¤‘ì¹˜(at)ë¥¼ ê³±í•©ë‹ˆë‹¤.
        # ì¤‘ìš”í•œ ì‹œì ì˜ ë°ì´í„°ëŠ” í¬ê²Œ ë‚¨ê³ , ë¶ˆí•„ìš”í•œ ì‹œì ì€ 0ì— ê°€ê¹ê²Œ ì‘ì•„ì§‘ë‹ˆë‹¤.
        context = inputs * at

        # [ë‹¨ê³„ 4] ì •ë³´ í•©ì‚° (Context Vector)
        # ê°€ì¤‘ì¹˜ê°€ ê³±í•´ì§„ ëª¨ë“  ì‹œì ì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤(Sum).
        # ê²°ê³¼ê°’ì€ (Batch, Feature_dim) í˜•íƒœì˜ 'ë¬¸ë§¥ ë²¡í„°'ê°€ ë©ë‹ˆë‹¤.
        # ê°€ì¤‘ì¹˜(at)ë„ ë‚˜ì¤‘ì— ì‹œê°í™”í•˜ê¸° ìœ„í•´ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
        return tf.reduce_sum(context, axis=1), at

    def get_config(self):
        """
        ë ˆì´ì–´ì˜ ì„¤ì • ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì´ í•¨ìˆ˜ê°€ ìˆì–´ì•¼ model.save()ë¡œ ì €ì¥ëœ ëª¨ë¸ì„ ë‚˜ì¤‘ì— ì™„ë²½íˆ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        config = super(AttentionLayer, self).get_config()
        # ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        return config
    
# 3. ëª¨ë¸ êµ¬ì¶• (Functional API ìŠ¤íƒ€ì¼ : Sequentialë³´ë‹¤ ìœ ì—°í•œ ëª¨ë¸ ì •ì˜ ë°©ì‹ì…ë‹ˆë‹¤)
inputs = Input(shape=(look_back, 1))
# LSTMì˜ ëª¨ë“  ì‹œì  ì¶œë ¥ì„ ìœ„í•´ return_sequences=True
#LSTM ê°ì²´ ìƒì„± í›„ ì…ë ¥ê°’ì„ ì „ë‹¬í•˜ì—¬ lstm_outì— ì €ì¥
lstm_out = LSTM(128, return_sequences=True)(inputs) 
# Attention ì ìš©
# AttentionLayer ê°ì²´ ìƒì„± í›„ lstm_outì„ ì…ë ¥ê°’ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ attention_outê³¼ attention_weightsì— ì €ì¥
attention_out, attention_weights = AttentionLayer()(lstm_out)

# ìµœì¢… ì¶œë ¥
# Dense ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ attention_outì—ì„œ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.
prediction = Dense(1)(attention_out)

model = Model(inputs=inputs, outputs=prediction)
model.compile(optimizer='adam', loss='mse')

# í•™ìŠµ
print("ğŸš€ Attention-LSTM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model.fit(X_train, y_train, epochs=300, batch_size=16, verbose=0)

# ëª¨ë¸ ì €ì¥ (.h5 ëŒ€ì‹  ìµœì‹  .keras í¬ë§· ê¶Œì¥í•˜ë‚˜ ì‚¬ìš©ì ì„¤ì •ì— ë§ì¶° .h5 ìœ ì§€)
model.save(os.path.join(save_dir, "air_passengers_best_model.h5"))

# 4. ì„±ëŠ¥ ê²€ì¦ ë° ì—­ë³€í™˜
y_pred_diff_scaled = model.predict(X_test)
y_pred_diff = scaler.inverse_transform(y_pred_diff_scaled).flatten()

# ì°¨ë¶„ ë°ì´í„° ë³µì› (ì´ì „ ì£¼ê¸° ê°’ + ì°¨ë¶„ ì˜ˆì¸¡ê°’)
actual_start_idx = len(passengers) - 24
y_pred_final = []
for i in range(24):
    prev_year_val = passengers[actual_start_idx - seasonal_period + i]
    y_pred_final.append(prev_year_val + y_pred_diff[i])

y_pred_final = np.array(y_pred_final)
y_actual_final = passengers[actual_start_idx:]

# MAPE ê³„ì‚°
mape = np.mean(np.abs((y_actual_final - y_pred_final) / y_actual_final)) * 100
print(f"ğŸ“Š ìµœì¢… ëª¨ë¸ MAPE: {mape:.2f}%")

# 5. ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 5))
plt.plot(y_actual_final, label='ì‹¤ì œê°’', marker='o', alpha=0.7)
plt.plot(y_pred_final, label=f'ì˜ˆì¸¡ê°’ (MAPE: {mape:.2f}%)', marker='x', color='red')
plt.title('í•­ê³µê¸° ìŠ¹ê° ìˆ˜ ì˜ˆì¸¡ (Attention-LSTM)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()