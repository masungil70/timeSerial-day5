# Attention(ì–´í…ì…˜) ë©”ì»¤ë‹ˆì¦˜

## 1. Attention(ì–´í…ì…˜) ë©”ì»¤ë‹ˆì¦˜ ì´ë€?

ì–´í…ì…˜ ì´ì „ì—ëŠ” ì£¼ë¡œ **RNN(ìˆœí™˜ ì‹ ê²½ë§)** ê¸°ë°˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ RNNì€ ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì•ë¶€ë¶„ì˜ ì •ë³´ë¥¼ ìŠì–´ë²„ë¦¬ëŠ” **ì¥ê¸° ì˜ì¡´ì„±(Long-term dependency)** ë¬¸ì œì™€, ëª¨ë“  ì •ë³´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ì— ì–µì§€ë¡œ êµ¬ê²¨ ë„£ì–´ì•¼ í•˜ëŠ” **ì •ë³´ ì†ì‹¤** ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

ì–´í…ì…˜ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **"ëª¨ë“  ë‹¨ì–´ë¥¼ ë˜‘ê°™ì´ ë³´ì§€ ë§ê³ , ì¶œë ¥ ë‹¨ì–´ì™€ ê´€ë ¨ ìˆëŠ” ì…ë ¥ ë‹¨ì–´ì— ì§‘ì¤‘(Attention)í•˜ì"** ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì¶œë°œí–ˆìŠµë‹ˆë‹¤.

ì‰½ê²Œ ë¹„ìœ í•˜ìë©´, ìš°ë¦¬ê°€ ë³µì¡í•œ ê·¸ë¦¼ì„ ë³¼ ë•Œ ì „ì²´ë¥¼ ë˜‘ê°™ì€ ê°•ë„ë¡œ ê´€ì°°í•˜ê¸°ë³´ë‹¤ **íŠ¹ì • ë¶€ë¶„(ì˜ˆ: ì‚¬ëŒì˜ ì–¼êµ´ì´ë‚˜ ì›€ì§ì´ëŠ” ë¬¼ì²´)** ì— ì‹œì„ ì„ ê³ ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

---

### 1. Attentionì˜ í•µì‹¬ ê°œë…

ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ë ¤ë©´ ì„¸ ê°€ì§€ ìš”ì†Œë¥¼ ê¼­ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ë§ˆì¹˜ ë„ì„œê´€ì—ì„œ ì±…ì„ ì°¾ëŠ” ê³¼ì •ê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤.

#### í•µì‹¬ ì›ë¦¬: Query, Key, Value

ì–´í…ì…˜ì€ ë³´í†µ ì„¸ ê°€ì§€ ìš”ì†Œì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:

* **Query (Q):** í˜„ì¬ ì°¾ê³ ì í•˜ëŠ” ì •ë³´ (ì§ˆë¬¸)
* **Key (K):** ë„ì„œê´€ì— ìˆëŠ” ì±…ë“¤ì˜ ì œëª© (ì¸ë±ìŠ¤/í‚¤ì›Œë“œ)
* **Value (V):** ì±… ì•ˆì— ë‹´ê¸´ ì‹¤ì œ ë‚´ìš© (ì •ë³´)

**ì‘ë™ ìˆœì„œ:**

1. **ìœ ì‚¬ë„ ê³„ì‚°:** í˜„ì¬ ì‹œì ì˜ `Query`ì™€ ëª¨ë“  `Key`ë¥¼ ë¹„êµí•˜ì—¬ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
2. **ì†Œí”„íŠ¸ë§¥ìŠ¤(Softmax):** ì´ ì ìˆ˜ë“¤ì„ í•©ì´ 1ì´ ë˜ë„ë¡ í™•ë¥ ê°’(ê°€ì¤‘ì¹˜)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
3. **ê°€ì¤‘í•© ê³„ì‚°:** êµ¬í•´ì§„ í™•ë¥ ê°’ì„ ê° `Value`ì— ê³±í•´ì„œ ëª¨ë‘ ë”í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ **ì¤‘ìš”í•œ ì •ë³´ëŠ” ì§„í•˜ê²Œ, ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì—°í•˜ê²Œ** ì„ì¸ í•˜ë‚˜ì˜ ë²¡í„°ê°€ ë‚˜ì˜µë‹ˆë‹¤.

ëª¨ë¸ì€ Queryì™€ ëª¨ë“  Key ì‚¬ì´ì˜ **ìœ ì‚¬ë„(Attention Score)** ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ì— ë”°ë¼ ê° Valueì— ë¶€ì—¬í•  ê°€ì¤‘ì¹˜ë¥¼ ê²°ì •í•˜ê³ , ì´ë“¤ì„ ëª¨ë‘ í•©ì³ì„œ ìµœì¢…ì ì¸ ì¶œë ¥ê°’ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.

---

### 2. ì£¼ìš” ì¥ì 

* **ì¥ê¸° ì˜ì¡´ì„±(Long-term Dependency) í•´ê²°:** ë¬¸ì¥ì´ ì•„ë¬´ë¦¬ ê¸¸ì–´ì ¸ë„ íŠ¹ì • ë‹¨ì–´ì™€ ê´€ë ¨ ìˆëŠ” ë‹¨ì–´ë¥¼ ì§ì ‘ ì—°ê²°í•´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ë¯€ë¡œ ì„±ëŠ¥ ì €í•˜ê°€ ì ìŠµë‹ˆë‹¤.
* **ì„±ëŠ¥ í–¥ìƒ:** ëª¨ë“  ë°ì´í„°ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  í•„ìš”í•œ ë¶€ë¶„ë§Œ ê³¨ë¼ í•™ìŠµí•˜ë¯€ë¡œ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.
* **ì„¤ëª… ê°€ëŠ¥ì„±(Interpretability):** ëª¨ë¸ì´ ê²°ê³¼ë¥¼ ë‚¼ ë•Œ **ì–´ë–¤ ë°ì´í„°ë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ ì‹œê°í™”** í•  ìˆ˜ ìˆì–´, AIì˜ íŒë‹¨ ê·¼ê±°ë¥¼ íŒŒì•…í•˜ê¸° ìœ ë¦¬í•©ë‹ˆë‹¤.

---

### 3. ì–´í…ì…˜ì˜ ì¢…ë¥˜

ì–´í…ì…˜ì€ ì ìš© ë°©ì‹ì— ë”°ë¼ ëª‡ ê°€ì§€ ì£¼ìš” í˜•íƒœë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

| ì¢…ë¥˜ | íŠ¹ì§• |
| --- | --- |
| **Dot-Product Attention** | Queryì™€ Keyë¥¼ ë‚´ì í•˜ì—¬ ë‹¨ìˆœí•˜ê²Œ ìœ ì‚¬ë„ë¥¼ êµ¬í•¨ (ê°€ì¥ ê¸°ë³¸) |
| **Bahdanau Attention** | RNNì˜ ì€ë‹‰ ìƒíƒœë¥¼ í™œìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚° |
| **Self-Attention** | **ìê¸° ìì‹ ** ë‚´ì˜ ë‹¨ì–´ë“¤ë¼ë¦¬ ê´€ê³„ë¥¼ íŒŒì•… (Transformerì˜ í•µì‹¬) |
| **Multi-Head Attention** | ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ íŒŒì•… |

### 4. êµ¬í˜„ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì 

ì‹¤ì œë¡œ ì–´í…ì…˜ì„ í™œìš©í•œ ëª¨ë¸ì„ ì„¤ê³„í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ ì  ìš”ì†Œê°€ í¬í•¨ë©ë‹ˆë‹¤.

1. **Scaling (ìŠ¤ì¼€ì¼ë§):** $Key$ì˜ ì°¨ì›ì´ ì»¤ì§€ë©´ ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì°¨ì› ìˆ˜($\sqrt{d_k}$)ë¡œ ë‚˜ëˆ„ì–´ ê°’ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
2. **Masking (ë§ˆìŠ¤í‚¹):** ë¯¸ë˜ì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ë³´ê³  ë‹µì„ ë² ë¼ì§€ ëª»í•˜ë„ë¡, ì•„ì§ ë‚˜ì˜¤ì§€ ì•Šì€ ì •ë³´ëŠ” ì–´í…ì…˜ ì ìˆ˜ë¥¼ ê°•ì œë¡œ 0ìœ¼ë¡œ ë§Œë“œëŠ” ì¥ì¹˜ë¥¼ ë‘¡ë‹ˆë‹¤.

---

### 5. ìš”ì•½: ìˆ˜í•™ì  êµ¬ì¡°

ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ (Scales Dot-Product Attention ê¸°ì¤€):

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

* $QK^T$ : ìœ ì‚¬ë„ ì¸¡ì •
* $\sqrt{d_k}$ : ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§
* $\text{softmax}$ : ê°€ì¤‘ì¹˜ í•©ì„ 1ë¡œ ë³€í™˜
* $V$ ê³±í•˜ê¸° : ìµœì¢… ì •ë³´ ì¶”ì¶œ

---

### 6. ì–´í…ì…˜ì´ ê°€ì ¸ì˜¨ ë³€í™”

* **ì„±ëŠ¥ í–¥ìƒ:** ê¸´ ë¬¸ì¥ì—ì„œë„ ë¬¸ë§¥ì„ ë†“ì¹˜ì§€ ì•Šê³  ì •í™•í•œ ë²ˆì—­/ìš”ì•½ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.
* **ì‹œê°í™” ê°€ëŠ¥:** ëª¨ë¸ì´ ì–´ë–¤ ë‹¨ì–´ì— ì§‘ì¤‘í•´ì„œ ê²°ê³¼ë¥¼ ë‚´ë†“ì•˜ëŠ”ì§€ 'ì–´í…ì…˜ ë§µ'ì„ í†µí•´ í™•ì¸í•˜ë©° ëª¨ë¸ì˜ íŒë‹¨ ê·¼ê±°ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **Transformerì˜ íƒ„ìƒ:** ì–´í…ì…˜ë§Œìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“  Transformerê°€ ë“±ì¥í•˜ë©°, í˜„ì¬ ìš°ë¦¬ê°€ ì“°ëŠ” **GPT, BERT** ê°™ì€ ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‹œëŒ€ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤.

### 7.í™œìš© ë¶„ì•¼

| ë¶„ì•¼ | í™œìš© ì‚¬ë¡€ |
| --- | --- |
| **ê¸°ê³„ ë²ˆì—­** | ì†ŒìŠ¤ ë¬¸ì¥ì—ì„œ ë²ˆì—­í•  ë‹¨ì–´ì™€ ê°€ì¥ ì—°ê´€ ê¹Šì€ ë‹¨ì–´ë¥¼ ì°¾ì•„ ë²ˆì—­ (ì˜ˆ: Google ë²ˆì—­) |
| **ë¬¸ì„œ ìš”ì•½** | ì „ì²´ ë³¸ë¬¸ ì¤‘ í•µì‹¬ ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ì—¬ ìš”ì•½ë¬¸ ìƒì„± |
| **ì´ë¯¸ì§€ ìº¡ì…”ë‹** | ì´ë¯¸ì§€ì˜ íŠ¹ì • ë¶€ë¶„ì„ ë³´ê³  ê·¸ì— ë§ëŠ” ì„¤ëª… ê¸€ì„ ìƒì„± |
| **ìƒì„±í˜• AI** | GPTì™€ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë§¥ë½ì— ë§ëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì‚¬ìš© |

---

### 8. í•­ê³µê¸° ìŠ¹ê° ë°ì´í„°ë¥¼ í™œìš©í•œ Attention ë©”ì»¤ë‹ˆì¦˜ ì˜ˆì œ

ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ 5ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ì— ë§ì¶°, **Attention-LSTM ê¸°ë°˜ í•­ê³µê° ìŠ¹ê° ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸**ì˜ ì „ì²´ ê³¼ì •ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

---

#### 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬ (Data Preparation)

ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

* **ê³„ì ˆì„± ì°¨ë¶„ (Seasonal Differencing):** í•­ê³µê° ë°ì´í„°ëŠ” 1ë…„ ì£¼ê¸°ì˜ ê°•í•œ ê³„ì ˆì„±ì„ ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í˜„ì¬ ì‹œì ì—ì„œ 12ê°œì›” ì „ì˜ ë°ì´í„°ë¥¼ ëº€ 'ì¦ê°ëŸ‰'ë§Œì„ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì•ˆì •í™”í•©ë‹ˆë‹¤.
* **ì •ê·œí™” (Normalization):** `MinMaxScaler`ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ 0~1 ì‚¬ì´ë¡œ ë³€í™˜í•˜ì—¬ ì‹ ê²½ë§ì˜ í•™ìŠµ ì†ë„ì™€ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.
* **ì‹œí€€ìŠ¤ ìƒì„±:** ê³¼ê±° 12ê°œì›”(`look_back`)ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¬¶ìŒìœ¼ë¡œ ë§Œë“¤ì–´ ë‹¤ìŒ ì‹œì ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì§€ë„ í•™ìŠµ í˜•íƒœë¡œ êµ¬ì¡°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.

#### 2ë‹¨ê³„: ëª¨ë¸ ì„¤ê³„ (Model Architecture)

LSTM ë ˆì´ì–´ ìœ„ì— **Attention ë ˆì´ì–´**ë¥¼ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°ì…ë‹ˆë‹¤.

* **LSTM ë ˆì´ì–´:** ì‹œê³„ì—´ ë°ì´í„°ì˜ ì¥ê¸°ì ì¸ ì˜ì¡´ì„±ì„ í•™ìŠµí•©ë‹ˆë‹¤. `return_sequences=True` ì„¤ì •ì„ í†µí•´ ëª¨ë“  ì‹œì ì˜ ì •ë³´ë¥¼ Attention ë ˆì´ì–´ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
* **Attention ë©”ì»¤ë‹ˆì¦˜ ì ìš© (í•µì‹¬):**
  * **ì™œ ì ìš©í–ˆëŠ”ê°€?** LSTMì€ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ê³¼ê±°ì˜ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìŠì–´ë²„ë¦¬ëŠ” 'ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ'ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Attentionì€ **ê³¼ê±° 12ê°œì›” ì¤‘ ì˜ˆì¸¡í•˜ë ¤ëŠ” ì‹œì ê³¼ ê°€ì¥ ìƒê´€ê´€ê³„ê°€ ë†’ì€ íŠ¹ì • ë‹¬(ì˜ˆ: ì‘ë…„ ë™ì›”)ì˜ ì •ë³´ì— ë” ì§‘ì¤‘**í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
  * **ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€?** ê°€ì¤‘ì¹˜(`W`)ì™€ í¸í–¥(`b`)ì„ í†µí•´ ê° ì‹œì ì˜ ì¤‘ìš”ë„(ì ìˆ˜)ë¥¼ ê³„ì‚°í•˜ê³ , `Softmax` í•¨ìˆ˜ë¥¼ í†µí•´ í•©ê³„ê°€ 1ì´ ë˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤. ì´ ê°€ì¤‘ì¹˜ë¥¼ ì›ë³¸ ë°ì´í„°ì— ê³±í•´ 'ë¬¸ë§¥ ë²¡í„°(Context Vector)'ë¥¼ ìƒì„±í•¨ìœ¼ë¡œì¨ ì¤‘ìš”í•œ ì‹œì ì˜ ì •ë³´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

#### 3ë‹¨ê³„: ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ (Compile & Training)

ì„¤ê³„í•œ ëª¨ë¸ì´ ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°ˆ ìˆ˜ ìˆë„ë¡ ì„¤ì •í•˜ê³  í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

* **ì†ì‹¤ í•¨ìˆ˜(Loss) ë° ì˜µí‹°ë§ˆì´ì €:** ì‹œê³„ì—´ ì˜ˆì¸¡(íšŒê·€)ì´ë¯€ë¡œ `mse`(í‰ê·  ì œê³± ì˜¤ì°¨)ë¥¼ ì‚¬ìš©í•˜ë©°, íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ `adam` ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.
* **í•™ìŠµ ì‹¤í–‰:** 300íšŒì˜ ì—í¬í¬(Epochs) ë™ì•ˆ ë°ì´í„°ë¥¼ ë°˜ë³µ í•™ìŠµí•˜ë©° ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ê°‘ë‹ˆë‹¤.
  * ì—í¬í¬(Epochs) íšŸìˆ˜ë¥¼ 50íšŒ ë°˜ë³µí•˜ëŠ” ê²ƒê³¼ ë¹„êµí•´ë³´ì„¸ìš”.

#### 4ë‹¨ê³„: ëª¨ë¸ í‰ê°€ ë° ê²€ì¦ (Evaluation)

í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ìµœê·¼ 24ê°œì›” ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •ë°€í•˜ê²Œ ì¸¡ì •í•©ë‹ˆë‹¤.

* **MAPE(í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨) í™•ì¸:** ì‹¤ì œê°’ ëŒ€ë¹„ ì˜¤ì°¨ê°€ ëª‡ %ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì œê³µëœ ìµœì¢… ê²°ê³¼ì— ë”°ë¥´ë©´ **MAPE 3.42%** ë¼ëŠ” ë§¤ìš° ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
* **ì‹œê°í™”:** ì‹¤ì œ ìŠ¹ê° ìˆ˜ ì¶”ì´ì™€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì¶”ì´ë¥¼ ê·¸ë˜í”„ë¡œ ë¹„êµí•˜ì—¬ íŒ¨í„´ì´ ì¼ì¹˜í•˜ëŠ”ì§€ ìœ¡ì•ˆìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.

#### 5ë‹¨ê³„: ëª¨ë¸ ì €ì¥ ë° í™œìš© (Saving & Inference)

ì„±ëŠ¥ì´ í™•ì¸ëœ ëª¨ë¸ì„ ì˜ˆì¸¡ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

* **ëª¨ë¸ ì €ì¥:** ì»¤ìŠ¤í…€ ë ˆì´ì–´ì¸ `AttentionLayer`ë¥¼ í¬í•¨í•˜ì—¬ ì „ì²´ ëª¨ë¸ì„ `.h5` í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
* **ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥:** ì˜ˆì¸¡ ì‹œ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì›ë˜ ë‹¨ìœ„(ìŠ¹ê° ìˆ˜)ë¡œ ë³µì›í•˜ê¸° ìœ„í•´ ì •ê·œí™” ê·œì¹™ì´ ë‹´ê¸´ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ `.pkl` íŒŒì¼ë¡œ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.

---

#### ì „ì²´ ì½”ë“œ

íŒŒì¼ëª… : day4/step4/attention-tranining.py

```python
import numpy as np
import pandas as pd
import koreanize_matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import  Layer, Input, LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
import os
import joblib

# 0. GPU ë° í˜¼í•© ì •ë°€ë„ ì„¤ì • (ì„ íƒ ì‚¬í•­)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
data_df = pd.read_csv('./data/flights.csv')
passengers = data_df['Passengers'].values.astype(float)

# ê³„ì ˆì„± ì°¨ë¶„ (Seasonal Differencing: í˜„ì¬ - 12ê°œì›” ì „)
seasonal_period = 12
diff_passengers = passengers[seasonal_period:] - passengers[:-seasonal_period]
diff_passengers = diff_passengers.reshape(-1, 1)

# ë°ì´í„° ì •ê·œí™”
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(diff_passengers)

# ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ê²½ë¡œ í™•ì¸
save_dir = './model'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
joblib.dump(scaler, os.path.join(save_dir, "air_passengers_scaler.pkl"))

# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_dataset(dataset, look_back=12):
    X, y = [], []
    for i in range(len(dataset) - look_back):
        X.append(dataset[i:(i + look_back), 0])
        y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(y)

look_back = 12
X, y = create_dataset(data_scaled, look_back)
X = X.reshape((X.shape[0], X.shape[1], 1))

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
train_size = len(X) - 24
X_train, X_test = X[:train_size], X[train_size:] # ë˜ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì¸ë±ì‹±
y_train, y_test = y[:train_size], y[train_size:]

# 2. Attention ë ˆì´ì–´ ì •ì˜ (Serialization ëŒ€ì‘ ìµœì í™”)
@tf.keras.utils.register_keras_serializable() # ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì‹œ ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì¸ì‹ì„ ìœ„í•œ ë°ì½”ë ˆì´í„°
class AttentionLayer(Layer):
    """
    LSTMì˜ ì¶œë ¥ ì‹œí€€ìŠ¤ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘(Attention)í•˜ì—¬ 
    ê°€ì¤‘ í•©ì‚°ëœ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ì»¤ìŠ¤í…€ ë ˆì´ì–´ì…ë‹ˆë‹¤.
    """
    def __init__(self, **kwargs):
        # ë¶€ëª¨ í´ë˜ìŠ¤(layers.Layer)ì˜ ì´ˆê¸°í™” ë£¨í‹´ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        """
        ë ˆì´ì–´ê°€ ì²˜ìŒ í˜¸ì¶œë  ë•Œ ì‹¤í–‰ë˜ë©°, í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜(Weight)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        input_shape:     (Batch_size, Time_steps, Input_dim) í˜•íƒœì…ë‹ˆë‹¤.
        ë°°ì—´ì„ ì™¼ìª½ì— ì ‘ê·¼   :  0           1           2
        ë°°ì—´ì„ ì™¼ë¥¸ìª½ì— ì ‘ê·¼ : -3          -2          -1
        """
        # 1. í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ W ì •ì˜: ê° ì‹œì ì˜ íŠ¹ì§•ê°’ì— ê³±í•´ì§ˆ ê°€ì¤‘ì¹˜ í–‰ë ¬
        # í˜•íƒœ: (ì…ë ¥ ì°¨ì›, 1) -> ê° ì‹œì ì˜ ë²¡í„°ë¥¼ ìŠ¤ì¹¼ë¼ ì ìˆ˜ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•¨
        self.W = self.add_weight(name="att_weight", 
                                 shape=(input_shape[-1], 1), 
                                 initializer="normal",
                                 trainable=True)
        
        # 2. í¸í–¥ b ì •ì˜: í™œì„±í™” í•¨ìˆ˜ ì ìš© ì „ ë”í•´ì§€ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ìƒìˆ˜
        # í˜•íƒœ: (íƒ€ì„ìŠ¤í… ìˆ˜, 1) -> ê° ì‹œì ë³„ë¡œ ê³ ìœ í•œ í¸í–¥ê°’ ë¶€ì—¬
        self.b = self.add_weight(name="att_bias", 
                                 shape=(input_shape[1], 1), 
                                 initializer="zeros",
                                 trainable=True)
        
        # ê°€ì¤‘ì¹˜ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŒì„ ì„ ì–¸í•©ë‹ˆë‹¤.
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        """
        ì‹¤ì œ ì—°ì‚°ì´ ì¼ì–´ë‚˜ëŠ” í•µì‹¬ ë©”ì„œë“œ (Forward Propagation)
        inputs: LSTMì˜ ì¶œë ¥ê°’ (Batch, Time_steps, Feature_dim)
        """
        # [ë‹¨ê³„ 1] ì ìˆ˜ ê³„ì‚° (Score Calculation)
        # inputs(W) + b ë¥¼ í†µí•´ ê° ì‹œì ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 'ì—ë„ˆì§€ ì ìˆ˜'ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        # tanh í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ë¥¼ -1ê³¼ 1 ì‚¬ì´ë¡œ ë¹„ì„ í˜• ë³€í™˜í•©ë‹ˆë‹¤.
        et = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)

        # [ë‹¨ê³„ 2] í™•ë¥  ë³€í™˜ (Attention Weights)
        # Softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì‹œì ì˜ et í•©ê³„ê°€ 1(100%)ì´ ë˜ë„ë¡ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        # axis=1ì€ íƒ€ì„ìŠ¤í… ë°©í–¥ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
        at = tf.nn.softmax(et, axis=1)

        # [ë‹¨ê³„ 3] ê°€ì¤‘ì¹˜ ì ìš© (Weight Application)
        # ì›ë³¸ ì…ë ¥ê°’(inputs)ì— ê³„ì‚°ëœ í™•ë¥  ê°€ì¤‘ì¹˜(at)ë¥¼ ê³±í•©ë‹ˆë‹¤.
        # ì¤‘ìš”í•œ ì‹œì ì˜ ë°ì´í„°ëŠ” í¬ê²Œ ë‚¨ê³ , ë¶ˆí•„ìš”í•œ ì‹œì ì€ 0ì— ê°€ê¹ê²Œ ì‘ì•„ì§‘ë‹ˆë‹¤.
        context = inputs * at

        # [ë‹¨ê³„ 4] ì •ë³´ í•©ì‚° (Context Vector)
        # ê°€ì¤‘ì¹˜ê°€ ê³±í•´ì§„ ëª¨ë“  ì‹œì ì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤(Sum).
        # ê²°ê³¼ê°’ì€ (Batch, Feature_dim) í˜•íƒœì˜ 'ë¬¸ë§¥ ë²¡í„°'ê°€ ë©ë‹ˆë‹¤.
        # ê°€ì¤‘ì¹˜(at)ë„ ë‚˜ì¤‘ì— ì‹œê°í™”í•˜ê¸° ìœ„í•´ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
        return tf.reduce_sum(context, axis=1), at

    def get_config(self):
        """
        ë ˆì´ì–´ì˜ ì„¤ì • ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì´ í•¨ìˆ˜ê°€ ìˆì–´ì•¼ model.save()ë¡œ ì €ì¥ëœ ëª¨ë¸ì„ ë‚˜ì¤‘ì— ì™„ë²½íˆ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        config = super(AttentionLayer, self).get_config()
        # ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        return config
    
# 3. ëª¨ë¸ êµ¬ì¶• (Functional API ìŠ¤íƒ€ì¼ : Sequentialë³´ë‹¤ ìœ ì—°í•œ ëª¨ë¸ ì •ì˜ ë°©ì‹ì…ë‹ˆë‹¤)
inputs = Input(shape=(look_back, 1))
# LSTMì˜ ëª¨ë“  ì‹œì  ì¶œë ¥ì„ ìœ„í•´ return_sequences=True
#LSTM ê°ì²´ ìƒì„± í›„ ì…ë ¥ê°’ì„ ì „ë‹¬í•˜ì—¬ lstm_outì— ì €ì¥
lstm_out = LSTM(128, return_sequences=True)(inputs) 
# Attention ì ìš©
# AttentionLayer ê°ì²´ ìƒì„± í›„ lstm_outì„ ì…ë ¥ê°’ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ attention_outê³¼ attention_weightsì— ì €ì¥
attention_out, attention_weights = AttentionLayer()(lstm_out)

# ìµœì¢… ì¶œë ¥
# Dense ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ attention_outì—ì„œ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.
prediction = Dense(1)(attention_out)

model = Model(inputs=inputs, outputs=prediction)
model.compile(optimizer='adam', loss='mse')

# í•™ìŠµ
print("ğŸš€ Attention-LSTM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model.fit(X_train, y_train, epochs=300, batch_size=16, verbose=0)

# ëª¨ë¸ ì €ì¥ (.h5 ëŒ€ì‹  ìµœì‹  .keras í¬ë§· ê¶Œì¥í•˜ë‚˜ ì‚¬ìš©ì ì„¤ì •ì— ë§ì¶° .h5 ìœ ì§€)
model.save(os.path.join(save_dir, "air_passengers_best_model.h5"))

# 4. ì„±ëŠ¥ ê²€ì¦ ë° ì—­ë³€í™˜
y_pred_diff_scaled = model.predict(X_test)
y_pred_diff = scaler.inverse_transform(y_pred_diff_scaled).flatten()

# ì°¨ë¶„ ë°ì´í„° ë³µì› (ì´ì „ ì£¼ê¸° ê°’ + ì°¨ë¶„ ì˜ˆì¸¡ê°’)
actual_start_idx = len(passengers) - 24
y_pred_final = []
for i in range(24):
    prev_year_val = passengers[actual_start_idx - seasonal_period + i]
    y_pred_final.append(prev_year_val + y_pred_diff[i])

y_pred_final = np.array(y_pred_final)
y_actual_final = passengers[actual_start_idx:]

# MAPE ê³„ì‚°
mape = np.mean(np.abs((y_actual_final - y_pred_final) / y_actual_final)) * 100
print(f"ğŸ“Š ìµœì¢… ëª¨ë¸ MAPE: {mape:.2f}%")

# 5. ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 5))
plt.plot(y_actual_final, label='ì‹¤ì œê°’', marker='o', alpha=0.7)
plt.plot(y_pred_final, label=f'ì˜ˆì¸¡ê°’ (MAPE: {mape:.2f}%)', marker='x', color='red')
plt.title('í•­ê³µê¸° ìŠ¹ê° ìˆ˜ ì˜ˆì¸¡ (Attention-LSTM)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```

#### ì‹¤í–‰ ê²°ê³¼

![alt text](image-9.png)

---

### 9. í•™ìŠµí•œ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ 1961ë…„ í•­ê³µê¸° ìŠ¹ê° ì˜ˆì¸¡í•˜ê¸°

**ì¸í¼ëŸ°ìŠ¤(ì¶”ë¡ ) í”„ë¡œì„¸ìŠ¤ ê°œë°œ 4ë‹¨ê³„**ì— ë§ì¶° ê° ê³¼ì •ê³¼ **Attention ë©”ì»¤ë‹ˆì¦˜ì˜ í™œìš© ë°©ì‹**ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

---

#### 1ë‹¨ê³„: ëª¨ë¸ ë° í™˜ê²½ ë¡œë“œ (Model & Environment Loading)

í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ ë„êµ¬ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ ì˜ˆì¸¡ ì¤€ë¹„ë¥¼ í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

* **ì»¤ìŠ¤í…€ ë ˆì´ì–´ ë“±ë¡:** `load_model` ì‹œ ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ í•™ìŠµ ë•Œ ì •ì˜í•œ `AttentionLayer` í´ë˜ìŠ¤ë¥¼ ë™ì¼í•˜ê²Œ ì„ ì–¸í•˜ê³ , `Custom` íŒ¨í‚¤ì§€ë¡œ ë“±ë¡í•©ë‹ˆë‹¤.
* **íŒŒì¼ ë¡œë“œ:** `tf.keras.models.load_model`ì„ ì‚¬ìš©í•´ í•™ìŠµëœ ëª¨ë¸(`.h5`)ì„ ë¶ˆëŸ¬ì˜¤ê³ , `joblib`ìœ¼ë¡œ ì •ê·œí™” ê·œì¹™ì´ ë‹´ê¸´ ìŠ¤ì¼€ì¼ëŸ¬(`.pkl`)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
* **ì»´íŒŒì¼ ì˜µì…˜ ì œì™¸:** ì˜ˆì¸¡ ì „ìš©ì´ë¯€ë¡œ `compile=False`ë¥¼ ì„¤ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ í•™ìŠµ ì„¤ì • ë¡œë“œ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.

#### 2ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (Data Preparation)

ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ê¸°ì¤€ì ì´ ë˜ëŠ” ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³  ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

* **ê³„ì ˆì„± ì°¨ë¶„ ì ìš©:** ëª¨ë¸ì´ í•™ìŠµí•œ ë°©ì‹ëŒ€ë¡œ í˜„ì¬ ìŠ¹ê° ìˆ˜ ë°ì´í„°ì—ì„œ 12ê°œì›” ì „ ë°ì´í„°ë¥¼ ëº€ 'ì¦ê°ëŸ‰'ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
* **ìŠ¤ì¼€ì¼ë§:** ë¡œë“œëœ `scaler`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦ê°ëŸ‰ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* **ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶”ì¶œ:** 1961ë…„ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ 1960ë…„ì˜ ë§ˆì§€ë§‰ 12ê°œì›”ë¶„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ 3ì°¨ì› ë°°ì—´(`1, 12, 1`)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

#### 3ë‹¨ê³„: ì¬ê·€ì  ì˜ˆì¸¡ ë° Attention í™œìš© (Recursive Prediction & Attention)

ê°€ì¥ í•µì‹¬ì ì¸ ë‹¨ê³„ë¡œ, ëª¨ë¸ì´ ê³¼ê±° ë°ì´í„°ë¥¼ ë¶„ì„í•´ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

* **Attention ë©”ì»¤ë‹ˆì¦˜ì˜ í™œìš©:**
  * **í•µì‹¬ ë‹¨ì„œ í¬ì°©:** ëª¨ë¸ ë‚´ë¶€ì˜ `AttentionLayer`ëŠ” ì…ë ¥ëœ 12ê°œì›” ë°ì´í„° ì¤‘ **í˜„ì¬ ì˜ˆì¸¡í•˜ë ¤ëŠ” ì‹œì ê³¼ ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ ê³¼ê±° ì‹œì (ì˜ˆ: ì‘ë…„ ë™ì›”ì˜ ì¦ê° íŒ¨í„´)** ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
  * **ë¬¸ë§¥ ë²¡í„°(Context Vector) ìƒì„±:** `tanh` í•¨ìˆ˜ë¡œ ê³„ì‚°ëœ ì ìˆ˜ë¥¼ `softmax`ë¡œ í™•ë¥ í™”í•˜ì—¬, ì¤‘ìš”í•œ ì •ë³´ëŠ” ê°•ì¡°í•˜ê³  ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆëŠ” ê±¸ëŸ¬ë‚¸ 'ì••ì¶•ëœ ì •ë³´'ë¥¼ ì˜ˆì¸¡ ë ˆì´ì–´ì— ì „ë‹¬í•©ë‹ˆë‹¤.

* **ì¬ê·€ì  ì—…ë°ì´íŠ¸:** ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 1ì›”ì˜ ê°’ì„ ë‹¤ì‹œ ì…ë ¥ ë°ì´í„°ì˜ ëì— ë„£ê³ , ê°€ì¥ ì˜¤ë˜ëœ ê°’ì„ ë¹¼ëŠ” 'ìœˆë„ìš° ìŠ¬ë¼ì´ë”©' ë°©ì‹ì„ í†µí•´ 12ì›”ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤.

#### 4ë‹¨ê³„: ê²°ê³¼ ë³µì› ë° ì‹œê°í™” (Post-processing & Visualization)

ëª¨ë¸ì˜ ì¶œë ¥ê°’(ì •ê·œí™”ëœ ì¦ê°ëŸ‰)ì„ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‹¤ì œ ê°’ìœ¼ë¡œ ë°”ê¾¸ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

* **ì—­ë³€í™˜(Inverse Transform):** `scaler`ë¥¼ í†µí•´ 0~1 ì‚¬ì´ì˜ ì˜ˆì¸¡ê°’ì„ ë‹¤ì‹œ ì‹¤ì œ 'ì¦ê°ëŸ‰' ìˆ˜ì¹˜ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.
* **ì°¨ë¶„ ë³µì›:** ì˜ˆì¸¡ëœ ì¦ê°ëŸ‰ì„ 1960ë…„ì˜ ì‹¤ì œ ê°’ì— ë”í•˜ì—¬ **ìµœì¢…ì ì¸ 1961ë…„ ìŠ¹ê° ìˆ˜** ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.
* **ê²°ê³¼ ì¶œë ¥:** `matplotlib`ì„ í†µí•´ ê³¼ê±° ë°ì´í„°ì™€ ì—°ê²°ëœ ë¯¸ë˜ ì˜ˆì¸¡ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê³ , ì›”ë³„ ì˜ˆìƒ ìŠ¹ê° ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

---

#### ğŸ’¡ ì¸í¼ëŸ°ìŠ¤ì—ì„œ Attentionì´ ì¤‘ìš”í•œ ì´ìœ 

í•™ìŠµ ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¸í¼ëŸ°ìŠ¤ì—ì„œë„ **Attention**ì€ ëª¨ë¸ì´ **"ê³¼ê±° 12ê°œì›” ì¤‘ ì–´ë””ë¥¼ ì¤‘ìš”í•˜ê²Œ ë´ì•¼ í•˜ëŠ”ê°€"** ë¥¼ ê³„ì†í•´ì„œ ê°€ì´ë“œí•©ë‹ˆë‹¤. íŠ¹íˆ ì¬ê·€ì  ì˜ˆì¸¡ì€ ë’¤ë¡œ ê°ˆìˆ˜ë¡ ì˜¤ì°¨ê°€ ëˆ„ì ë  ìœ„í—˜ì´ ìˆëŠ”ë°, Attention ë©”ì»¤ë‹ˆì¦˜ì´ **ê³ ì •ëœ ì£¼ê¸°ì˜ í•µì‹¬ íŒ¨í„´ì— ì§‘ì¤‘**í•˜ê²Œ í•¨ìœ¼ë¡œì¨ 1961ë…„ ì˜ˆì¸¡ì—ì„œë„ ì¼ê´€ì„± ìˆëŠ” ê³„ì ˆì  íë¦„ì„ ìœ ì§€í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

#### ì „ì²´ ì½”ë“œ

íŒŒì¼ëª… : day4/step4/attention-inference.py

```python
import numpy as np
import pandas as pd
import koreanize_matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import joblib
import os

# 1. Attention ë ˆì´ì–´ í´ë˜ìŠ¤ ì •ì˜ (ë¡œë“œ ì‹œ í•„ìˆ˜)
# @tf.keras.utils.register_keras_serializable()ëŠ” í•™ìŠµ ì‹œ ë“±ë¡ëœ ì´ë¦„ì„ ì°¾ê¸° ìœ„í•´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
@tf.keras.utils.register_keras_serializable(package="Custom")
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1), initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        et = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)
        at = tf.nn.softmax(et, axis=1)
        context = inputs * at
        return tf.reduce_sum(context, axis=1), at

    def get_config(self):
        return super(AttentionLayer, self).get_config()

# 2. íŒŒì¼ ê²½ë¡œ ì„¤ì •
model_path = './model/air_passengers_best_model.h5'
scaler_path = './model/air_passengers_scaler.pkl'
data_path = './data/flights.csv'

# íŒŒì¼ ì¡´ì¬ í™•ì¸
if not all(os.path.exists(f) for f in [model_path, scaler_path, data_path]):
    print("âŒ í•„ìš”í•œ íŒŒì¼(ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ë°ì´í„°)ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    # 3. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    # 'Custom>AttentionLayer' ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ custom_object_scopeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    custom_objects = {'AttentionLayer': AttentionLayer}
    
    with tf.keras.utils.custom_object_scope(custom_objects):
        model = tf.keras.models.load_model(model_path, compile=False)
    
    scaler = joblib.load(scaler_path)
    print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")

    # 4. ì›ë³¸ ë°ì´í„° ë¡œë“œ (ë¯¸ë˜ ì˜ˆì¸¡ì˜ ê¸°ì¤€ì )
    data_df = pd.read_csv(data_path)
    col_name = 'passengers' if 'passengers' in data_df.columns else 'Passengers'
    passengers = data_df[col_name].values.astype(float)

    # 5. ë¯¸ë˜ ì˜ˆì¸¡ (1961ë…„ 12ê°œì›”)
    # ë§ˆì§€ë§‰ 12ê°œì›”ì˜ ì°¨ë¶„ ë°ì´í„° ì¤€ë¹„
    seasonal_period = 12
    diff_passengers = passengers[seasonal_period:] - passengers[:-seasonal_period]
    diff_scaled = scaler.transform(diff_passengers.reshape(-1, 1))
    
    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ (1960ë…„ íŒ¨í„´)
    current_batch = diff_scaled[-12:].reshape(1, 12, 1)
    
    future_diff_preds = []
    print("ğŸ”® 1961ë…„ ë¯¸ë˜ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
    
    for i in range(12):
        pred_scaled = model.predict(current_batch, verbose=0)
        future_diff_preds.append(pred_scaled[0, 0])
        
        # ìœˆë„ìš° ìŠ¬ë¼ì´ë”© ì—…ë°ì´íŠ¸
        new_val = pred_scaled.reshape(1, 1, 1)
        current_batch = np.append(current_batch[:, 1:, :], new_val, axis=1)

    # 6. ì—­ë³€í™˜ ë° ë³µì›
    future_diff_unscaled = scaler.inverse_transform(np.array(future_diff_preds).reshape(-1, 1)).flatten()
    
    # 1961ë…„ ìµœì¢…ê°’ = 1960ë…„ ì‹¤ì œê°’ + ì˜ˆì¸¡ëœ ì¦ê°ëŸ‰
    last_year_1960 = passengers[-12:]
    forecast_1961 = last_year_1960 + future_diff_unscaled

    # 7. ì‹œê°í™” ë° ì¶œë ¥
    future_months = pd.date_range(start='1961-01-01', periods=12, freq='MS')
    forecast_series = pd.Series(forecast_1961, index=future_months)

    plt.figure(figsize=(12, 6))
    plt.plot(pd.to_datetime(data_df['Month'])[-24:], passengers[-24:], label='ì‹¤ì œê°’ (1959-1960)', marker='o')
    plt.plot(forecast_series, label='ì˜ˆì¸¡ê°’ (1961)', marker='x', color='red', linestyle='--')
    plt.title('í•­ê³µê¸° ìŠ¹ê° ìˆ˜ ë¯¸ë˜ ì˜ˆì¸¡ (1961ë…„)')
    plt.ylabel('ìŠ¹ê° ìˆ˜')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    print("\n--- 1961ë…„ ì˜ˆì¸¡ ê²°ê³¼ ---")
    for month, val in zip(future_months, forecast_1961):
        print(f"{month.strftime('%Y-%m')}: {int(val)}ëª…")
```

#### ì‹¤í–‰ ê²°ê³¼

![alt text](image-10.png)

---