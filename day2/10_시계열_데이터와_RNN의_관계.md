# 시계열 데이터와 RNN의 관계

시계열 데이터와 RNN(순환 신경망)은 마치 **'흐르는 물'** 과 **'그 물의 흐름을 기억하는 그릇'** 의 관계와 같습니다. 시계열 데이터가 가진 가장 큰 특징인 '시간적 순서'를 가장 잘 처리할 수 있도록 설계된 모델이 바로 RNN이기 때문입니다.

---

## 1. 시계열 데이터의 핵심: "과거가 미래를 결정한다"

시계열 데이터(Time-Series Data)는 일정한 시간 간격으로 배치된 데이터들의 수열입니다. 주가, 날씨, 음성 신호 등이 대표적이죠. 이 데이터들의 공통점은 **현재의 값이 이전 값들의 영향을 강하게 받는다**는 점입니다.

일반적인 인공 신경망(ANN)은 각 데이터를 독립적으로 처리하지만, 시계열 데이터에서는 데이터 사이의 **'맥락(Context)'** 을 파악하는 것이 무엇보다 중요합니다.

---

## 2. RNN(Recurrent Neural Network)의 작동 원리

RNN의 '순환(Recurrent)'은 정보가 루프를 돌며 전달된다는 뜻입니다.

* **기억하는 신경망:** RNN은 특정 시점($t$)의 입력을 처리할 때, 이전 시점($t-1$)에서 계산된 정보를 함께 입력받습니다. 이를 **은닉 상태(Hidden State)** 라고 부르며, 일종의 '단기 기억' 역할을 합니다.
* **구조적 특징:**
1. 현재의 입력 ($x_t$)
2. 이전의 기억 ($h_{t-1}$)
3. 위 두 가지를 조합해 현재의 기억 ($h_t$)과 결과값 ($y_t$)을 생성

$$h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$$

이 수식처럼 RNN은 매 순간 과거의 기억을 업데이트하며 다음 단계를 예측합니다.

---

## 3. 실생활 예시: 문장 완성 (NLP, 자연어 처리)

시계열 데이터의 대표적인 예인 '텍스트'를 통해 RNN의 역할을 살펴봅시다.

> **"나는 배가 고파서 [ ]을 먹었다."**

1. **'나는'** 입력: 모델이 주어를 인식합니다.
2. **'배가'** 입력: 앞서 나온 '나'와 연결하여 상태를 저장합니다.
3. **'고파서'** 입력: 원인(배고픔)을 기억에 추가합니다.
4. **결과 도출:** 앞선 맥락(나+배고픔)을 통해 빈칸에 '자동차'나 '구름'이 아닌 **'밥'** 이나 **'사과'** 같은 단어가 올 확률이 높다는 것을 계산해 냅니다.

---

## 4. RNN의 한계와 발전 (LSTM, GRU)

기본적인 RNN은 치명적인 약점이 있습니다. 문장이 너무 길어지면 앞부분의 정보를 잊어버리는 **장기 의존성(Long-Term Dependency)** 문제, 즉 '건망증' 증세가 나타납니다.

이를 보완하기 위해 중요한 정보는 오래 기억하고 불필요한 정보는 삭제하는 장치를 추가한 **LSTM(Long Short-Term Memory)** 이나 **GRU** 같은 변형 모델들이 시계열 분석 현장에서 더 활발히 사용되고 있습니다.

---

## 요약

| 구분 | 일반 신경망 (DNN) | 순환 신경망 (RNN) |
| --- | --- | --- |
| **데이터 관계** | 데이터 간 독립적임 | 이전 데이터가 다음 데이터에 영향 |
| **기억 장치** | 없음 (휘발성) | 은닉 상태(Hidden State) 보유 |
| **주요 용도** | 이미지 분류, 정적인 데이터 | 주가 예측, 번역, 음성 인식 |

