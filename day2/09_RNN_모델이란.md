# RNN 모델이란?

RNN(Recurrent Neural Network, 순환 신경망)은 시퀀스 데이터, 즉 **순서가 중요한 데이터**를 처리하기 위해 설계된 인공신경망의 한 종류입니다. 일반적인 신경망이 데이터를 한 번에 입력받고 끝내는 것과 달리, RNN은 이전 단계의 정보를 기억해 다음 단계로 전달하는 '기억력'을 가지고 있다는 게 핵심입니다.

---

## 1. RNN의 핵심 구조

RNN의 가장 큰 특징은 **'순환(Recurrent)'** 구조입니다.

일반적인 DNN(Deep Neural Network)은 데이터가 입력층에서 출력층으로 한 방향으로만 흐릅니다. 하지만 RNN은 출력의 일부가 다시 입력으로 들어가는 **순환(Recurrence) 구조**를 가집니다.

* **히든 스테이트(Hidden State):** 이전 단계의 정보를 저장하는 일종의 '메모리' 역할을 합니다.
* **시계열 데이터 처리:** 문장(단어의 나열), 주식 차트(시간에 따른 가격), 음성 데이터처럼 앞뒤 맥락이 중요한 데이터를 다루기에 최적화되어 있습니다.

---

## 2. RNN의 작동 원리

RNN은 각 시점($t$)마다 입력을 받아 상태를 업데이트합니다. 이를 수식으로 나타내면 다음과 같습니다.

### 수학적 구조

현재 시점 $t$에서의 은닉 상태 $h_t$는 이전 시점의 은닉 상태 $h_{t-1}$와 현재 입력 $x_t$의 함수로 계산됩니다.

$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b)$$

- $W_{hh}$: 이전 은닉 상태를 위한 가중치
- $W_{xh}$: 현재 입력을 위한 가중치
- $b$: 편향(bias)
- $\tanh$: 활성화 함수 (정보의 범위를 -1에서 1 사이로 조정)

## 3. RNN의 한계점

RNN은 이론적으로는 훌륭하지만, 실제 아주 긴 문장이나 데이터를 처리할 때는 치명적인 단점이 있습니다.

* **기울기 소실 문제 (Vanishing Gradient Problem):** 문장이 길어질수록 앞부분의 정보가 뒤로 전달되지 않고 희미해지는 현상입니다.
* **장기 의존성 문제:** 예를 들어, "내가 어릴 때 살던 고향의 이름은... (중략) ... 이다"라는 문장에서 아주 멀리 떨어진 단어 사이의 관계를 파악하기 힘들어합니다.

---

## 4. RNN의 진화: LSTM과 GRU

이러한 한계를 극복하기 위해 정보를 선택적으로 기억하거나 삭제하는 **게이트(Gate)** 구조가 추가된 모델들이 등장했습니다.

1. **LSTM (Long Short-Term Memory):** '망각 게이트' 등을 통해 어떤 정보를 오래 기억할지 결정합니다. 현대 RNN 계열 중 가장 많이 쓰입니다.
2. **GRU (Gated Recurrent Unit):** LSTM의 구조를 단순화하여 계산 효율을 높인 모델입니다.

---

## 5. RNN이 주로 쓰이는 곳

* **자연어 처리 (NLP):** 기계 번역, 챗봇, 감성 분석
* **시계열 예측:** 주가 예측, 날씨 예보
* **음성 인식:** 실시간 통역기, AI 스피커

## 6. RNN의 주요 모델 종류

| 모델명 | 특징 | 주요 용도 |
| --- | --- | --- |
| **Vanilla RNN** | 가장 기본적인 형태. 짧은 시퀀스에는 좋으나 '장기 의존성' 문제(과거 정보를 잊음)가 있음. | 간단한 시계열 예측 |
| **LSTM (Long Short-Term Memory)** | RNN의 치명적인 단점인 기울기 소실 문제를 해결하기 위해 'Gate' 구조를 도입. | 긴 문장 번역, 음성 인식 |
| **GRU (Gated Recurrent Unit)** | LSTM을 간소화한 모델. 성능은 비슷하면서 연산 속도가 더 빠름. | 실시간 텍스트 생성 |

## 7. 한계점과 발전

RNN은 뒤로 갈수록 앞부분의 정보를 잊어버리는 **장기 의존성(Long-term Dependency)** 문제 때문에 최근에는 **트랜스포머(Transformer)** 모델로 많이 대체되었습니다. 우리가 잘 아는 ChatGPT도 이 트랜스포머 기반입니다. 하지만 실시간 데이터 처리나 가벼운 모델이 필요한 곳에서는 여전히 RNN 계열이 활약하고 있습니다.
